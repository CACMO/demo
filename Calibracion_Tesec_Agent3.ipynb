{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMkozj6G1aRM/nco3Izcx9j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CACMO/demo/blob/main/Calibracion_Tesec_Agent3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UkaxOz-3NCi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6925b052"
      },
      "source": [
        "# Task\n",
        "Analyze the semiconductor calibration results from '/content/manufacturing_data.CSV' by loading the data, performing initial data inspection (displaying first few rows, column names, and data types), calculating and displaying descriptive statistics for numerical columns, visualizing value distributions using histograms or box plots with appropriate legends, and finally, summarizing the initial findings and observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7c2a95f"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "### Subtask:\n",
        "Load the semiconductor calibration results from '/content/manufacturing_data.CSV' into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "361697d1"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I'll import the pandas library to enable data manipulation. Then, I'll load the specified CSV file into a DataFrame named `df` and display its first few rows to verify successful loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fe4af8e"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ParserError` indicates malformed lines in the CSV file that are causing the loading to fail. By setting `on_bad_lines='skip'`, pandas will ignore these problematic lines, allowing the rest of the data to be loaded. Using `engine='python'` is necessary for `on_bad_lines` parameter to work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "383a7ecf"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/HCT#100 FT CUC 1mohm 1.5% Antes.CSV', sep=None, engine='python', on_bad_lines='skip')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f756c529"
      },
      "source": [
        "## Initial Data Inspection\n",
        "\n",
        "### Subtask:\n",
        "Display the first few rows, column names, and data types to understand the structure of the calibration data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1edc80e"
      },
      "source": [
        "**Reasoning**:\n",
        "To understand the structure of the loaded data, I will display the first few rows, the column names, and their respective data types using pandas functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1c81bab"
      },
      "source": [
        "print(\"First few rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns)\n",
        "print(\"\\nData types and non-null values:\")\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1967e336"
      },
      "source": [
        "## Data Preprocessing (Correcting Data Loading)\n",
        "\n",
        "### Subtask:\n",
        "Reload the data, skipping initial metadata rows, to correctly load the tabular calibration results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bf1d0fe"
      },
      "source": [
        "**Reasoning**:\n",
        "To correctly load the tabular data, I need to skip the initial metadata rows. Based on the previous error, I will use `skiprows=15` to start reading the CSV from line 16, and then display the head, columns, and info of the new DataFrame to verify the loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abca1906"
      },
      "source": [
        "import pandas as pd\n",
        "df_calibration = pd.read_csv('//content/HCT#100 FT CUC 1mohm 1.5% Antes.CSV', skiprows=15)\n",
        "print(\"First few rows of the corrected DataFrame:\")\n",
        "print(df_calibration.head())\n",
        "print(\"\\nColumn names:\")\n",
        "print(df_calibration.columns)\n",
        "print(\"\\nData types and non-null values:\")\n",
        "df_calibration.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a6caf8f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the header row (line 16 of the original CSV) to extract column names\n",
        "# skiprows=15 gets to line 16, nrows=1 reads only that line.\n",
        "header_df = pd.read_csv('/content/HCT#100 FT CUC 1mohm 1.5% Antes.CSV', skiprows=15, nrows=1, header=None, engine='python')\n",
        "\n",
        "# Extract potential column names from header_df, starting from the third column (index 2)\n",
        "# The first column will be a test identifier, the second is often empty.\n",
        "new_columns_raw = list(header_df.iloc[0, 2:].values)\n",
        "\n",
        "# Ensure column names are unique by appending numerical suffixes\n",
        "unique_columns = []\n",
        "counts = {}\n",
        "for col in new_columns_raw:\n",
        "    col = str(col).strip()\n",
        "    if col in counts:\n",
        "        counts[col] += 1\n",
        "        unique_columns.append(f\"{col}_{counts[col]}\")\n",
        "    else:\n",
        "        counts[col] = 0 # Initialize count for first occurrence\n",
        "        unique_columns.append(col) # No suffix for the first occurrence\n",
        "\n",
        "# The final list of column names: 'Test_Type' for the first column,\n",
        "# 'Unnamed_Col' for the second (often empty), followed by the unique test names.\n",
        "new_columns = ['Test_Type', 'Unnamed_Col'] + unique_columns\n",
        "\n",
        "# Step 2: Load the actual calibration data\n",
        "# Skip 30 rows to start reading from line 31 of the original CSV (where the actual numerical data begins).\n",
        "# header=None indicates that there's no header row in the data being read at skiprows=30,\n",
        "# and names=new_columns assigns our constructed column names.\n",
        "df_calibration_cleaned = pd.read_csv(\n",
        "    '/content/HCT#100 FT CUC 1mohm 1.5% Antes.CSV',\n",
        "    skiprows=30, # Skip 15 (initial metadata) + 1 (header row) + 14 (further metadata) = 30 rows\n",
        "    header=None, # No header row in the data part\n",
        "    names=new_columns # Assign the unique column names\n",
        ")\n",
        "\n",
        "print(\"First few rows of the truly tabular DataFrame:\")\n",
        "print(df_calibration_cleaned.head())\n",
        "print(\"\\nColumn names:\")\n",
        "print(df_calibration_cleaned.columns)\n",
        "print(\"\\nData types and non-null values:\")\n",
        "df_calibration_cleaned.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f33f9b40"
      },
      "source": [
        "## Calculate Descriptive Statistics\n",
        "\n",
        "### Subtask:\n",
        "Compute and display descriptive statistics (mean, standard deviation, min, max, etc.) for the numerical columns in the calibration data to get an overview of the value distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65b54225"
      },
      "source": [
        "**Reasoning**:\n",
        "To get an overview of the value distributions, I will identify all numerical columns in the `df_calibration_cleaned` DataFrame and then calculate their descriptive statistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a05e2d66"
      },
      "source": [
        "print(\"Descriptive statistics for numerical columns:\")\n",
        "# Identify numerical columns\n",
        "numerical_columns = df_calibration_cleaned.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate descriptive statistics\n",
        "descriptive_stats = numerical_columns.describe()\n",
        "\n",
        "# Display the results\n",
        "print(descriptive_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5098b8f"
      },
      "source": [
        "## Visualize Value Distributions\n",
        "\n",
        "### Subtask:\n",
        "Generate histograms or box plots for key numerical columns to visualize the distribution of calibration values and identify any potential outliers or patterns. Include appropriate legends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f76dd71"
      },
      "source": [
        "**Reasoning**:\n",
        "To visualize the distribution of calibration values, I will generate histograms and box plots for the specified key numerical columns, ensuring each plot has appropriate labels and titles for clarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8473b42"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select a few representative numerical columns for visualization\n",
        "# Corrected selected_columns to use existing column names from '1' to '48'.\n",
        "selected_columns = ['1', '2', '40', '45', '48']\n",
        "\n",
        "# --- Generate Histograms ---\n",
        "print(\"Generating Histograms for selected numerical columns...\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(selected_columns):\n",
        "    plt.subplot(2, 3, i + 1) # Arrange in 2 rows, 3 columns\n",
        "    sns.histplot(df_calibration_cleaned[col], kde=True)\n",
        "    plt.title(f'Histogram of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Generate Box Plots ---\n",
        "print(\"\\nGenerating Box Plots for selected numerical columns...\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(selected_columns):\n",
        "    plt.subplot(2, 3, i + 1) # Arrange in 2 rows, 3 columns\n",
        "    sns.boxplot(y=df_calibration_cleaned[col])\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "    plt.ylabel(col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "def load_calibration_data(file_path):\n",
        "    \"\"\"Load and process calibration data from a CSV file.\"\"\"\n",
        "    try:\n",
        "        # Skip the first 15 rows to get to the relevant data\n",
        "        data = pd.read_csv(file_path, skiprows=15, header=None)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_calibration(data, tolerance=0.015):\n",
        "    \"\"\"Analyze calibration data and check against tolerance.\"\"\"\n",
        "    results = []\n",
        "    critical_tests = []\n",
        "\n",
        "    for i, row in data.iterrows():\n",
        "        if pd.notna(row[2]):  # Assuming the measurements start from the third column\n",
        "            value = row[2]\n",
        "            # Calculate error percentage\n",
        "            # Here, you would typically compare against a reference value\n",
        "            # For demonstration, assume reference is the first value\n",
        "            if i == 0:\n",
        "                reference = value\n",
        "            error = abs(value - reference) / reference * 100\n",
        "\n",
        "            if error > tolerance:\n",
        "                critical_tests.append((i, error))\n",
        "\n",
        "            results.append({\n",
        "                'test_id': i,\n",
        "                'value': value,\n",
        "                'error': error,\n",
        "                'within_tolerance': error <= tolerance\n",
        "            })\n",
        "\n",
        "    # Sort critical tests by descending error\n",
        "    critical_tests.sort(key=lambda x: x[1], reverse=True)\n",
        "    return results, critical_tests[:10]\n",
        "\n",
        "def compare_calibrations(initial_data, adjusted_data):\n",
        "    \"\"\"Compare initial and adjusted calibration data.\"\"\"\n",
        "    initial_results, initial_critical = analyze_calibration(initial_data)\n",
        "    adjusted_results, adjusted_critical = analyze_calibration(adjusted_data)\n",
        "\n",
        "    # Calculate overall improvement metrics\n",
        "    improvement = sum(1 for i, a in zip(initial_results, adjusted_results) if a['error'] < i['error'])\n",
        "    avg_initial_error = sum(r['error'] for r in initial_results) / len(initial_results)\n",
        "    avg_adjusted_error = sum(r['error'] for r in adjusted_results) / len(adjusted_results)\n",
        "\n",
        "    return improvement, avg_initial_error, avg_adjusted_error, initial_critical, adjusted_critical\n",
        "\n",
        "def visualize_critical_tests(initial_critical, adjusted_critical):\n",
        "    \"\"\"Visualize the most critical tests.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.title('Critical Tests Comparison')\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(range(len(initial_critical)), [err for _, err in initial_critical])\n",
        "    plt.title('Initial Calibration')\n",
        "    plt.xlabel('Test ID')\n",
        "    plt.ylabel('Error (%)')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(range(len(adjusted_critical)), [err for _, err in adjusted_critical])\n",
        "    plt.title('Adjusted Calibration')\n",
        "    plt.xlabel('Test ID')\n",
        "    plt.ylabel('Error (%)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def generate_report(initial_results, adjusted_results, critical_initial, critical_adjusted, engineer_name):\n",
        "    \"\"\"Generate a detailed report.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"calibration_analysis_{timestamp}.txt\"\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(f\"Calibration Analysis Report - {timestamp}\\n\")\n",
        "        f.write(f\"Engineer: {engineer_name}\\n\\n\")\n",
        "\n",
        "        f.write(\"Initial Calibration Results:\\n\")\n",
        "        for result in initial_results:\n",
        "            status = \"Within tolerance\" if result['within_tolerance'] else \"OUT OF TOLERANCE\"\n",
        "            f.write(f\"Test {result['test_id']}: Value={result['value']}, Error={result['error']:.2f}% {status}\\n\")\n",
        "\n",
        "        f.write(\"\\nAdjusted Calibration Results:\\n\")\n",
        "        for result in adjusted_results:\n",
        "            status = \"Within tolerance\" if result['within_tolerance'] else \"OUT OF TOLERANCE\"\n",
        "            f.write(f\"Test {result['test_id']}: Value={result['value']}, Error={result['error']:.2f}% {status}\\n\")\n",
        "\n",
        "        f.write(\"\\nCritical Tests Initial:\\n\")\n",
        "        for test_id, error in critical_initial:\n",
        "            f.write(f\"Test {test_id}: {error:.2f}%\\n\")\n",
        "\n",
        "        f.write(\"\\nCritical Tests Adjusted:\\n\")\n",
        "        for test_id, error in critical_adjusted:\n",
        "            f.write(f\"Test {test_id}: {error:.2f}%\\n\")\n",
        "\n",
        "        f.write(\"\\nSummary:\\n\")\n",
        "        f.write(f\"Total Tests: {len(initial_results)}\\n\")\n",
        "        f.write(f\"Initial Out of Tolerance: {len([r for r in initial_results if not r['within_tolerance']])}\\n\")\n",
        "        f.write(f\"Adjusted Out of Tolerance: {len([r for r in adjusted_results if not r['within_tolerance']])}\\n\")\n",
        "\n",
        "    print(f\"Report saved as {filename}\")\n",
        "\n",
        "def main():\n",
        "    print(\"Calibration Analysis Tool\")\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "    initial_data = None\n",
        "    adjusted_data = None\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nMenu:\")\n",
        "        print(\"1. Load Initial Calibration Data\")\n",
        "        print(\"2. Load Adjusted Calibration Data\")\n",
        "        print(\"3. Analyze Initial Calibration\")\n",
        "        print(\"4. Analyze Adjusted Calibration\")\n",
        "        print(\"5. Compare Calibrations\")\n",
        "        print(\"6. Visualize Critical Tests\")\n",
        "        print(\"7. Generate Report\")\n",
        "        print(\"8. Exit\")\n",
        "\n",
        "        choice = input(\"Choose an option: \")\n",
        "\n",
        "        if choice == '1':\n",
        "            file_path = input(\"Enter path to initial calibration file: \")\n",
        "            initial_data = load_calibration_data(file_path)\n",
        "            if initial_data is not None:\n",
        "                print(\"Initial calibration data loaded successfully.\")\n",
        "        elif choice == '2':\n",
        "            file_path = input(\"Enter path to adjusted calibration file: \")\n",
        "            adjusted_data = load_calibration_data(file_path)\n",
        "            if adjusted_data is not None:\n",
        "                print(\"Adjusted calibration data loaded successfully.\")\n",
        "        elif choice == '3':\n",
        "            if initial_data is not None:\n",
        "                results, critical = analyze_calibration(initial_data)\n",
        "                print(\"\\nInitial Calibration Analysis:\")\n",
        "                for result in results:\n",
        "                    print(f\"Test {result['test_id']}: {result['value']}, Error={result['error']:.2f}% { 'OK' if result['within_tolerance'] else 'FAIL' }\")\n",
        "            else:\n",
        "                print(\"No initial calibration data loaded.\")\n",
        "        elif choice == '4':\n",
        "            if adjusted_data is not None:\n",
        "                results, critical = analyze_calibration(adjusted_data)\n",
        "                print(\"\\nAdjusted Calibration Analysis:\")\n",
        "                for result in results:\n",
        "                    print(f\"Test {result['test_id']}: {result['value']}, Error={result['error']:.2f}% { 'OK' if result['within_tolerance'] else 'FAIL' }\")\n",
        "            else:\n",
        "                print(\"No adjusted calibration data loaded.\")\n",
        "        elif choice == '5':\n",
        "            if initial_data is not None and adjusted_data is not None:\n",
        "                improvement, initial_avg, adjusted_avg, initial_critical, adjusted_critical = compare_calibrations(initial_data, adjusted_data)\n",
        "                print(f\"\\nImprovement: {improvement} tests improved\")\n",
        "                print(f\"Initial Average Error: {initial_avg:.2f}%\")\n",
        "                print(f\"Adjusted Average Error: {adjusted_avg:.2f}%\")\n",
        "            else:\n",
        "                print(\"Both datasets must be loaded to compare.\")\n",
        "        elif choice == '6':\n",
        "            if initial_data is not None and adjusted_data is not None:\n",
        "                visualize_critical_tests(initial_data, adjusted_data)\n",
        "            else:\n",
        "                print(\"Both datasets must be loaded to visualize.\")\n",
        "        elif choice == '7':\n",
        "            engineer_name = input(\"Enter your name: \")\n",
        "            if initial_data is not None and adjusted_data is not None:\n",
        "                initial_results, initial_critical = analyze_calibration(initial_data)\n",
        "                adjusted_results, adjusted_critical = analyze_calibration(adjusted_data)\n",
        "                generate_report(initial_results, adjusted_results, initial_critical, adjusted_critical, engineer_name)\n",
        "            else:\n",
        "                print(\"Both datasets must be loaded to generate report.\")\n",
        "        elif choice == '8':\n",
        "            print(\"Exiting the program.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please choose a valid option.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "q715rqJw99Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8949b5fd"
      },
      "source": [
        "# Task\n",
        "Refine the data loading and analysis process by updating the interactive menu to prompt the user for the data type (Alta Corriente, Mainframe, or Produccion) when loading initial or adjusted calibration files, and ensure that the `load_calibration_data` function correctly parses the selected file based on its specified type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49fc929f"
      },
      "source": [
        "## Update Interactive Menu for File Type Selection\n",
        "\n",
        "### Subtask:\n",
        "Integrate a new prompt into the 'Load Initial Calibration Data' and 'Load Adjusted Calibration Data' options within the `main` function (interactive menu). Before file upload or path entry, the user will be asked to select the type of data being loaded: 'Alta Corriente', 'Mainframe', or 'Produccion'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb6e9d36"
      },
      "source": [
        "**Reasoning**:\n",
        "To implement the subtask, I need to modify the `main` function to prompt the user for the data type and pass it to the `load_calibration_data` function. I also need to update the `load_calibration_data` function to accept this new parameter and, more importantly, to correctly parse the numerical data from the CSV, converting string values to numeric to fix the `TypeError` observed previously. Finally, I will adjust the `visualize_critical_tests` function to correctly use the output from `analyze_calibration`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuración inicial\n",
        "TOLERANCE_PERCENT = 1.5  # Tolerancia por defecto\n",
        "\n",
        "# Función para cargar y procesar los datos\n",
        "def load_calibration_data(file_path, file_type='Alta Corriente'):\n",
        "    \"\"\"Cargar y procesar datos de calibración de diferentes formatos.\"\"\"\n",
        "    try:\n",
        "        if file_type == 'Mainframe':\n",
        "            header_df = pd.read_csv(file_path, skiprows=25, nrows=1, header=None, sep=';', engine='python')\n",
        "            data = pd.read_csv(file_path, skiprows=30, header=None, sep=';', engine='python')\n",
        "        else:\n",
        "            header_df = pd.read_csv(file_path, skiprows=15, nrows=1, header=None, engine='python')\n",
        "            data = pd.read_csv(file_path, skiprows=30, header=None, engine='python')\n",
        "\n",
        "        # Procesar nombres de columnas dinámicamente\n",
        "        new_columns_raw = list(header_df.iloc[0, 2:].values)\n",
        "        unique_columns = []\n",
        "        counts = {}\n",
        "        for col in new_columns_raw:\n",
        "            col = str(col).strip()\n",
        "            if col in counts:\n",
        "                counts[col] += 1\n",
        "                unique_columns.append(f\"{col}_{counts[col]}\")\n",
        "            else:\n",
        "                counts[col] = 0\n",
        "                unique_columns.append(col)\n",
        "        new_columns = ['Test_Type', 'Unnamed_Col'] + unique_columns\n",
        "\n",
        "        # Renombrar columnas\n",
        "        data.columns = new_columns\n",
        "\n",
        "        # Convertir columnas relevantes a numéricas\n",
        "        for col in new_columns[2:]:\n",
        "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el archivo {file_type}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Función para analizar la calibración con tolerancia personalizada\n",
        "def analyze_calibration(data, tolerance=TOLERANCE_PERCENT):\n",
        "    \"\"\"Analizar calibración con una tolerancia específica.\"\"\"\n",
        "    results = []\n",
        "    critical_tests = []\n",
        "    stats = {'total_tests': 0, 'within_tolerance': 0, 'out_of_tolerance': 0}\n",
        "\n",
        "    if data is None or not isinstance(data, pd.DataFrame):\n",
        "        print(\"Datos de calibración inválidos.\")\n",
        "        return results, critical_tests, stats\n",
        "\n",
        "    reference = None\n",
        "    for index, row in data.iterrows():\n",
        "        if pd.notna(row.iloc[2]):\n",
        "            value = row.iloc[2]\n",
        "            stats['total_tests'] += 1\n",
        "            if reference is None:\n",
        "                reference = value\n",
        "\n",
        "            if reference == 0:\n",
        "                error = float('inf') if value != 0 else 0\n",
        "            else:\n",
        "                error = abs(value - reference) / reference * 100\n",
        "\n",
        "            status = \"Dentro de tolerancia\" if error <= tolerance else \"Fuera de tolerancia\"\n",
        "            results.append({\n",
        "                'test_id': index,\n",
        "                'value': value,\n",
        "                'error': error,\n",
        "                'status': status\n",
        "            })\n",
        "\n",
        "            if error > tolerance:\n",
        "                critical_tests.append((index, error))\n",
        "                stats['out_of_tolerance'] += 1\n",
        "            else:\n",
        "                stats['within_tolerance'] += 1\n",
        "\n",
        "    stats['avg_error'] = sum(r['error'] for r in results) / len(results) if results else 0\n",
        "    critical_tests.sort(key=lambda x: x[1], reverse=True)\n",
        "    return results, critical_tests[:10], stats\n",
        "\n",
        "# Función para comparar calibraciones\n",
        "def compare_calibrations(initial_data, adjusted_data):\n",
        "    \"\"\"Comparar calibraciones iniciales y ajustadas.\"\"\"\n",
        "    initial_results, initial_critical, initial_stats = analyze_calibration(initial_data)\n",
        "    adjusted_results, adjusted_critical, adjusted_stats = analyze_calibration(adjusted_data)\n",
        "\n",
        "    comparison_stats = {\n",
        "        'improvement': sum(1 for i, a in zip(initial_results, adjusted_results) if a['error'] < i['error']),\n",
        "        'initial_avg_error': initial_stats['avg_error'],\n",
        "        'adjusted_avg_error': adjusted_stats['avg_error'],\n",
        "        'initial_out_of_tolerance': initial_stats['out_of_tolerance'],\n",
        "        'adjusted_out_of_tolerance': adjusted_stats['out_of_tolerance'],\n",
        "        'differences': [],\n",
        "        'similarities': []\n",
        "    }\n",
        "\n",
        "    for i, a in zip(initial_results, adjusted_results):\n",
        "        if i['test_id'] == a['test_id']:\n",
        "            if i['status'] != a['status']:\n",
        "                comparison_stats['differences'].append(f\"Test {i['test_id']}: {i['status']} -> {a['status']}\")\n",
        "            else:\n",
        "                comparison_stats['similarities'].append(f\"Test {i['test_id']}: {i['status']} (Sin cambios)\")\n",
        "\n",
        "    return comparison_stats\n",
        "\n",
        "# Función para visualizar comparación\n",
        "def visualize_comparison(comparison_stats):\n",
        "    \"\"\"Visualizar comparación de calibraciones.\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(['Inicio', 'Fin'], [comparison_stats['initial_avg_error'], comparison_stats['adjusted_avg_error']])\n",
        "    plt.title('Error Promedio')\n",
        "    plt.xlabel('Etapa')\n",
        "    plt.ylabel('Error (%)')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(['Errores iniciales', 'Errores finales'], [comparison_stats['initial_out_of_tolerance'], comparison_stats['adjusted_out_of_tolerance']])\n",
        "    plt.title('Número de Errores Fuera de Tolerancia')\n",
        "    plt.xlabel('Etapa')\n",
        "    plt.ylabel('Cantidad')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Función para generación de informe\n",
        "def generate_report(comparison_stats, engineer_name):\n",
        "    \"\"\"Generar informe detallado de comparación.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"calibration_comparison_report_{timestamp}.txt\"\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(f\"Informe de Comparación de Calibración - {timestamp}\\n\")\n",
        "        f.write(f\"Ingeniero: {engineer_name}\\n\\n\")\n",
        "\n",
        "        f.write(\"Estadísticas de Calibración Inicial:\\n\")\n",
        "        for key, value in comparison_stats['initial_stats'].items():\n",
        "            f.write(f\"{key.title().replace('_', ' ')}: {value}\\n\")\n",
        "\n",
        "        f.write(\"\\nEstadísticas de Calibración Ajustada:\\n\")\n",
        "        for key, value in comparison_stats['adjusted_stats'].items():\n",
        "            f.write(f\"{key.title().replace('_', ' ')}: {value}\\n\")\n",
        "\n",
        "        f.write(\"\\nDiferencias:\\n\")\n",
        "        for difference in comparison_stats['differences']:\n",
        "            f.write(f\"- {difference}\\n\")\n",
        "\n",
        "        f.write(\"\\nSimilitudes:\\n\")\n",
        "        for similarity in comparison_stats['similarities']:\n",
        "            f.write(f\"- {similarity}\\n\")\n",
        "\n",
        "        f.write(f\"\\nMejora en la Calibración:\\n\")\n",
        "        f.write(f\"Número de pruebas mejoradas: {comparison_stats['improvement']}\\n\")\n",
        "        f.write(f\"Redución promedio del error: {comparison_stats['initial_avg_error'] - comparison_stats['adjusted_avg_error']:.2f}%\\n\")\n",
        "\n",
        "    print(f\"Informe generado correctamente: {filename}\")\n",
        "\n",
        "# Función para visualización de SPC\n",
        "def visualize_spccomparison(initial_critical, adjusted_critical):\n",
        "    \"\"\"Visualizar comparación de SPC.\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    if initial_critical:\n",
        "        test_ids_initial = [str(t_id) for t_id, _ in initial_critical]\n",
        "        errors_initial = [err for _, err in initial_critical]\n",
        "        plt.bar(test_ids_initial, errors_initial, color='skyblue')\n",
        "        plt.title('Pruebas Iniciales Críticas')\n",
        "        plt.xlabel('ID de Prueba')\n",
        "        plt.ylabel('Error (%)')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No hay pruebas iniciales críticas.\", ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if adjusted_critical:\n",
        "        test_ids_adjusted = [str(t_id) for t_id, _ in adjusted_critical]\n",
        "        errors_adjusted = [err for _, err in adjusted_critical]\n",
        "        plt.bar(test_ids_adjusted, errors_adjusted, color='lightcoral')\n",
        "        plt.title('Pruebas Ajustadas Críticas')\n",
        "        plt.xlabel('ID de Prueba')\n",
        "        plt.ylabel('Error (%)')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No hay pruebas ajustadas críticas.\", ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Función para correlación\n",
        "def calculate_correlation(data):\n",
        "    \"\"\"Calcular correlación entre columnas numéricas.\"\"\"\n",
        "    numerical_data = data.select_dtypes(include=['int64', 'float64'])\n",
        "    correlation_matrix = numerical_data.corr()\n",
        "    return correlation_matrix\n",
        "\n",
        "def visualize_correlation(correlation_matrix):\n",
        "    \"\"\"Visualizar matriz de correlación.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n",
        "    plt.title('Matriz de Correlación')\n",
        "    plt.show()\n",
        "\n",
        "# Manejo de errores mejorado\n",
        "def handle_errors(func):\n",
        "    \"\"\"Decorador para manejar errores.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            print(f\"Error en {func.__name__}: {e}\")\n",
        "            return None\n",
        "    return wrapper\n",
        "\n",
        "# Menú interactivo\n",
        "def main():\n",
        "    print(\"Herramienta de Análisis de Calibración\")\n",
        "    print(\"------------------------------------\")\n",
        "\n",
        "    initial_data = None\n",
        "    adjusted_data = None\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nMenú:\")\n",
        "        print(\"1. Cargar Datos de Calibración Inicial\")\n",
        "        print(\"2. Cargar Datos de Calibración Ajustada\")\n",
        "        print(\"3. Analizar Calibración Inicial\")\n",
        "        print(\"4. Analizar Calibración Ajustada\")\n",
        "        print(\"5. Comparar Calibraciones\")\n",
        "        print(\"6. Visualizar SPC\")\n",
        "        print(\"7. Generar Informe\")\n",
        "        print(\"8. Visualizar Correlación\")\n",
        "        print(\"9. Salir\")\n",
        "\n",
        "        choice = input(\"Seleccione una opción: \")\n",
        "\n",
        "        if choice == '1':\n",
        "            file_type = input(\"Seleccione el tipo de archivo ('Alta Corriente', 'Mainframe'): \")\n",
        "            file_path = input(\"Ingrese la ruta del archivo de calibración inicial: \")\n",
        "            initial_data = load_calibration_data(file_path, file_type)\n",
        "            if initial_data is not None:\n",
        "                print(\"Datos cargados correctamente.\")\n",
        "        elif choice == '2':\n",
        "            file_type = input(\"Seleccione el tipo de archivo ('Alta Corriente', 'Mainframe'): \")\n",
        "            file_path = input(\"Ingrese la ruta del archivo de calibración ajustada: \")\n",
        "            adjusted_data = load_calibration_data(file_path, file_type)\n",
        "            if adjusted_data is not None:\n",
        "                print(\"Datos cargados correctamente.\")\n",
        "        elif choice == '3':\n",
        "            if initial_data is not None:\n",
        "                results, critical, stats = analyze_calibration(initial_data)\n",
        "                print(\"\\nResultados de la calibración inicial:\")\n",
        "                for result in results:\n",
        "                    print(f\"Prueba {result['test_id']}: {result['value']}, Error={result['error']:.2f}% {result['status']}\")\n",
        "                print(\"\\nEstadísticas:\")\n",
        "                for key, value in stats.items():\n",
        "                    print(f\"{key.title().replace('_', ' ')}: {value}\")\n",
        "            else:\n",
        "                print(\"No se han cargado los datos de calibración inicial.\")\n",
        "        elif choice == '4':\n",
        "            if adjusted_data is not None:\n",
        "                results, critical, stats = analyze_calibration(adjusted_data)\n",
        "                print(\"\\nResultados de la calibración ajustada:\")\n",
        "                for result in results:\n",
        "                    print(f\"Prueba {result['test_id']}: {result['value']}, Error={result['error']:.2f}% {result['status']}\")\n",
        "                print(\"\\nEstadísticas:\")\n",
        "                for key, value in stats.items():\n",
        "                    print(f\"{key.title().replace('_', ' ')}: {value}\")\n",
        "            else:\n",
        "                print(\"No se han cargado los datos de calibración ajustada.\")\n",
        "        elif choice == '5':\n",
        "            if initial_data is not None and adjusted_data is not None:\n",
        "                comparison_stats = compare_calibrations(initial_data, adjusted_data)\n",
        "                print(\"\\nEstadísticas de comparación:\")\n",
        "                print(f\"Mejora en pruebas: {comparison_stats['improvement']}\")\n",
        "                print(f\"Error promedio inicial: {comparison_stats['initial_avg_error']:.2f}%\")\n",
        "                print(f\"Error promedio ajustado: {comparison_stats['adjusted_avg_error']:.2f}%\")\n",
        "                visualize_comparison(comparison_stats)\n",
        "            else:\n",
        "                print(\"Necesita cargar ambos conjuntos de datos para realizar la comparación.\")\n",
        "        elif choice == '6':\n",
        "            if initial_data is not None and adjusted_data is not None:\n",
        "                initial_stats = analyze_calibration(initial_data)[2]\n",
        "                initial_critical = analyze_calibration(initial_data)[1]\n",
        "                adjusted_stats = analyze_calibration(adjusted_data)[2]\n",
        "                adjusted_critical = analyze_calibration(adjusted_data)[1]\n",
        "                visualize_spccomparison(initial_critical, adjusted_critical)\n",
        "            else:\n",
        "                print(\"Necesita cargar ambos conjuntos de datos para visualizar el SPC.\")\n",
        "        elif choice == '7':\n",
        "            engineer_name = input(\"Ingrese su nombre: \")\n",
        "            if initial_data is not None and adjusted_data is not None:\n",
        "                comparison_stats = compare_calibrations(initial_data, adjusted_data)\n",
        "                generate_report(comparison_stats, engineer_name)\n",
        "            else:\n",
        "                print(\"Necesita cargar ambos conjuntos de datos para generar el informe.\")\n",
        "        elif choice == '8':\n",
        "            if initial_data is not None:\n",
        "                correlation_matrix = calculate_correlation(initial_data)\n",
        "                visualize_correlation(correlation_matrix)\n",
        "            else:\n",
        "                print(\"Necesita cargar los datos para calcular la correlación.\")\n",
        "        elif choice == '9':\n",
        "            print(\"Saliendo del programa.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Opción inválida. Por favor seleccione una opción válida.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "iZJEhX-bfWnc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}