{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMKhfuf/db8mng7FNG7YFvd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CACMO/demo/blob/main/Tesec_Calibracion_Agente_WDeeps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMK61HskX84l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "class TesecAnalysisAgent:\n",
        "    def __init__(self):\n",
        "        self.initial_calibration = None\n",
        "        self.adjusted_calibration = None\n",
        "        self.engineer_name = None\n",
        "\n",
        "    def load_initial_calibration(self, file_path):\n",
        "        \"\"\"Cargar archivo de calibración inicial.\"\"\"\n",
        "        try:\n",
        "            self.initial_calibration = pd.read_csv(file_path)\n",
        "            print(\"Archivo de calibración inicial cargado correctamente.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error al cargar el archivo: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_adjusted_calibration(self, file_path):\n",
        "        \"\"\"Cargar archivo de calibración ajustada.\"\"\"\n",
        "        try:\n",
        "            self.adjusted_calibration = pd.read_csv(file_path)\n",
        "            print(\"Archivo de calibración ajustada cargado correctamente.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error al cargar el archivo: {e}\")\n",
        "            return False\n",
        "\n",
        "    def analyze_calibration(self, data, tolerance=0.015):\n",
        "        \"\"\"Analizar calibración y verificar tolerancia.\"\"\"\n",
        "        results = []\n",
        "        critical_tests = []\n",
        "\n",
        "        for i, row in data.iterrows():\n",
        "            if not pd.isna(row['values']):\n",
        "                error = row['error_percentage']\n",
        "                if error > tolerance:\n",
        "                    critical_tests.append((i, error))\n",
        "                results.append({\"test_id\": i, \"error\": error, \"within_tolerance\": error <= tolerance})\n",
        "\n",
        "        critical_tests.sort(key=lambda x: x[1], reverse=True)\n",
        "        return results, critical_tests[:10]\n",
        "\n",
        "    def compare_calibrations(self, initial, adjusted, tolerance=0.015):\n",
        "        \"\"\"Comparar las dos calibraciones.\"\"\"\n",
        "        initial_results, initial_critical = self.analyze_calibration(initial, tolerance)\n",
        "        adjusted_results, adjusted_critical = self.analyze_calibration(adjusted, tolerance)\n",
        "\n",
        "        improvement = sum(1 for i, a in zip(initial_results, adjusted_results) if a['error'] < i['error'])\n",
        "        average_improvement = (sum(a['error'] for a in adjusted_results) -\n",
        "                              sum(i['error'] for i in initial_results)) / len(initial_results)\n",
        "\n",
        "        return improvement, average_improvement, initial_critical, adjusted_critical\n",
        "\n",
        "    def visualize_critical_tests(self, critical_tests, title=\"\"):\n",
        "        \"\"\"Visualizar las pruebas más críticas.\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.title(title)\n",
        "        plt.bar(range(len(critical_tests)), [error for _, error in critical_tests])\n",
        "        plt.xlabel(\"Prueba\")\n",
        "        plt.ylabel(\"Porcentaje de Error\")\n",
        "        plt.show()\n",
        "\n",
        "    def generate_report(self, initial_results, adjusted_results, critical_initial, critical_adjusted):\n",
        "        \"\"\"Generar reporte final.\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"calibration_report_{timestamp}.txt\"\n",
        "\n",
        "        with open(filename, \"w\") as f:\n",
        "            f.write(f\"Informe de Calibración - {timestamp}\\n\")\n",
        "            f.write(f\"Ingeniero encargado: {self.engineer_name}\\n\\n\")\n",
        "\n",
        "            f.write(\"Resultados Iniciales:\\n\")\n",
        "            for result in initial_results:\n",
        "                status = \"Dentro de tolerancia\" if result['within_tolerance'] else \"FUERA DE TOLERANCIA\"\n",
        "                f.write(f\"Prueba {result['test_id']}: {result['error']:.2%} {status}\\n\")\n",
        "\n",
        "            f.write(\"\\nResultados Ajustados:\\n\")\n",
        "            for result in adjusted_results:\n",
        "                status = \"Dentro de tolerancia\" if result['within_tolerance'] else \"FUERA DE TOLERANCIA\"\n",
        "                f.write(f\"Prueba {result['test_id']}: {result['error']:.2%} {status}\\n\")\n",
        "\n",
        "            f.write(\"\\nPruebas Críticas Iniciales:\\n\")\n",
        "            for test_id, error in critical_initial:\n",
        "                f.write(f\"Prueba {test_id}: {error:.2%}\\n\")\n",
        "\n",
        "            f.write(\"\\nPruebas Críticas Ajustadas:\\n\")\n",
        "            for test_id, error in critical_adjusted:\n",
        "                f.write(f\"Prueba {test_id}: {error:.2%}\\n\")\n",
        "\n",
        "            f.write(\"\\nResumen:\\n\")\n",
        "            if any(not r['within_tolerance'] for r in initial_results):\n",
        "                f.write(\"La calibración inicial está FUERA DE TOLERANCIA.\\n\")\n",
        "            else:\n",
        "                f.write(\"La calibración inicial está DENTRO DE TOLERANCIA.\\n\")\n",
        "\n",
        "            if any(not r['within_tolerance'] for r in adjusted_results):\n",
        "                f.write(\"La calibración ajustada está FUERA DE TOLERANCIA.\\n\")\n",
        "            else:\n",
        "                f.write(\"La calibración ajustada está DENTRO DE TOLERANCIA.\\n\")\n",
        "\n",
        "def main():\n",
        "    agent = TesecAnalysisAgent()\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nMenú de Análisis de Calibración\")\n",
        "        print(\"1. Cargar archivo de calibración inicial\")\n",
        "        print(\"2. Analizar calibración inicial\")\n",
        "        print(\"3. Cargar archivo de calibración ajustada\")\n",
        "        print(\"4. Analizar calibración ajustada\")\n",
        "        print(\"5. Comparar calibraciones\")\n",
        "        print(\"6. Visualizar pruebas críticas\")\n",
        "        print(\"7. Generar informe final\")\n",
        "        print(\"8. Salir\")\n",
        "\n",
        "        option = input(\"Seleccione una opción: \")\n",
        "\n",
        "        if option == \"1\":\n",
        "            file_path = input(\"Ingrese la ruta del archivo de calibración inicial: \")\n",
        "            agent.load_initial_calibration(file_path)\n",
        "        elif option == \"2\":\n",
        "            if agent.initial_calibration is not None:\n",
        "                results, critical = agent.analyze_calibration(agent.initial_calibration)\n",
        "                print(\"Resultados de la calibración inicial:\")\n",
        "                for result in results:\n",
        "                    print(f\"Prueba {result['test_id']}: {result['error']:.2%} {'dentro' if result['within_tolerance'] else 'fuera'} de tolerancia\")\n",
        "            else:\n",
        "                print(\"No se ha cargado el archivo de calibración inicial.\")\n",
        "        elif option == \"3\":\n",
        "            file_path = input(\"Ingrese la ruta del archivo de calibración ajustada: \")\n",
        "            agent.load_adjusted_calibration(file_path)\n",
        "        elif option == \"4\":\n",
        "            if agent.adjusted_calibration is not None:\n",
        "                results, critical = agent.analyze_calibration(agent.adjusted_calibration)\n",
        "                print(\"Resultados de la calibración ajustada:\")\n",
        "                for result in results:\n",
        "                    print(f\"Prueba {result['test_id']}: {result['error']:.2%} {'dentro' if result['within_tolerance'] else 'fuera'} de tolerancia\")\n",
        "            else:\n",
        "                print(\"No se ha cargado el archivo de calibración ajustada.\")\n",
        "        elif option == \"5\":\n",
        "            if agent.initial_calibration is not None and agent.adjusted_calibration is not None:\n",
        "                improvement, avg_imp, crit_initial, crit_adj = agent.compare_calibrations(agent.initial_calibration, agent.adjusted_calibration)\n",
        "                print(f\"Mejora promedio: {avg_imp:.2%}\")\n",
        "                print(f\"Pruebas mejoradas: {improvement}\")\n",
        "                print(\"Pruebas críticas iniciales:\")\n",
        "                for test_id, error in crit_initial:\n",
        "                    print(f\"Prueba {test_id}: {error:.2%}\")\n",
        "                print(\"Pruebas críticas ajustadas:\")\n",
        "                for test_id, error in crit_adj:\n",
        "                    print(f\"Prueba {test_id}: {error:.2%}\")\n",
        "            else:\n",
        "                print(\"Ambos archivos deben estar cargados para 비교lar.\")\n",
        "        elif option == \"6\":\n",
        "            # This line needs to be fixed too: agent.visualize_critical_tests(agent.initial_calibration)\n",
        "            # The `visualize_critical_tests` method expects a list of critical tests, not the entire DataFrame.\n",
        "            # For now, I'm just fixing the primary syntax error.\n",
        "            if agent.initial_calibration is not None:\n",
        "                _, initial_critical = agent.analyze_calibration(agent.initial_calibration)\n",
        "                agent.visualize_critical_tests(initial_critical, title=\"Pruebas Críticas Iniciales\")\n",
        "            else:\n",
        "                print(\"No se ha cargado el archivo de calibración inicial.\")\n",
        "        elif option == \"7\":\n",
        "            agent.engineer_name = input(\"Ingrese el nombre del ingeniero encargado: \")\n",
        "            if agent.initial_calibration is not None and agent.adjusted_calibration is not None:\n",
        "                initial_results, critical_initial = agent.analyze_calibration(agent.initial_calibration)\n",
        "                adjusted_results, critical_adjusted = agent.analyze_calibration(agent.adjusted_calibration)\n",
        "                agent.generate_report(initial_results, adjusted_results, critical_initial, critical_adjusted)\n",
        "                print(\"Informe generado correctamente.\")\n",
        "            else:\n",
        "                print(\"Ambos archivos de calibración deben estar cargados para generar el informe.\")\n",
        "        elif option == \"8\":\n",
        "            break\n",
        "        else:\n",
        "            print(\"Opción inválida. Por favor seleccione una opción válida.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}