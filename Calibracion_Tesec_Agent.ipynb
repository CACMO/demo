{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPdJxS1rpDW4J3dHVpQDVj9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CACMO/demo/blob/main/Calibracion_Tesec_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UkaxOz-3NCi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6925b052"
      },
      "source": [
        "# Task\n",
        "Analyze the semiconductor calibration results from '/content/manufacturing_data.CSV' by loading the data, performing initial data inspection (displaying first few rows, column names, and data types), calculating and displaying descriptive statistics for numerical columns, visualizing value distributions using histograms or box plots with appropriate legends, and finally, summarizing the initial findings and observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7c2a95f"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "### Subtask:\n",
        "Load the semiconductor calibration results from '/content/manufacturing_data.CSV' into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "361697d1"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I'll import the pandas library to enable data manipulation. Then, I'll load the specified CSV file into a DataFrame named `df` and display its first few rows to verify successful loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44e244aa"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/manufacturing_data.CSV')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e2f5352"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the CSV file failed with a `ParserError`, suggesting an issue with the delimiter or file structure. By setting `sep=None` and `engine='python'`, pandas will try to automatically detect the appropriate delimiter, which is a more flexible approach to handle irregularly formatted files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "608f1d2a"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/manufacturing_data.CSV', sep=None, engine='python')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fe4af8e"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ParserError` indicates malformed lines in the CSV file that are causing the loading to fail. By setting `on_bad_lines='skip'`, pandas will ignore these problematic lines, allowing the rest of the data to be loaded. Using `engine='python'` is necessary for `on_bad_lines` parameter to work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "383a7ecf"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/manufacturing_data.CSV', sep=None, engine='python', on_bad_lines='skip')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f756c529"
      },
      "source": [
        "## Initial Data Inspection\n",
        "\n",
        "### Subtask:\n",
        "Display the first few rows, column names, and data types to understand the structure of the calibration data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1edc80e"
      },
      "source": [
        "**Reasoning**:\n",
        "To understand the structure of the loaded data, I will display the first few rows, the column names, and their respective data types using pandas functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1c81bab"
      },
      "source": [
        "print(\"First few rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns)\n",
        "print(\"\\nData types and non-null values:\")\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1967e336"
      },
      "source": [
        "## Data Preprocessing (Correcting Data Loading)\n",
        "\n",
        "### Subtask:\n",
        "Reload the data, skipping initial metadata rows, to correctly load the tabular calibration results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bf1d0fe"
      },
      "source": [
        "**Reasoning**:\n",
        "To correctly load the tabular data, I need to skip the initial metadata rows. Based on the previous error, I will use `skiprows=15` to start reading the CSV from line 16, and then display the head, columns, and info of the new DataFrame to verify the loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abca1906"
      },
      "source": [
        "import pandas as pd\n",
        "df_calibration = pd.read_csv('/content/manufacturing_data.CSV', skiprows=15)\n",
        "print(\"First few rows of the corrected DataFrame:\")\n",
        "print(df_calibration.head())\n",
        "print(\"\\nColumn names:\")\n",
        "print(df_calibration.columns)\n",
        "print(\"\\nData types and non-null values:\")\n",
        "df_calibration.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c14a0b2f"
      },
      "source": [
        "**Reasoning**:\n",
        "To correctly load the tabular calibration results, I need to identify and use the appropriate header row and skip all preceding metadata rows. Based on the previous inspection, the meaningful column names (e.g., 'CONT', 'IGSS', 'SAME') are found in the 16th line of the original CSV file (row index 0 of the current `df_calibration`), and the actual data starts after 5 additional metadata rows (lines 'Item' through 'Bias 1'). Therefore, I will extract column names from the 16th line and then reload the CSV skipping the first 20 lines (15 + 5 metadata rows) to get the clean tabular data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63e85eb0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extract potential column names from the first meaningful row (line 16 of original CSV)\n",
        "# This is df_calibration.iloc[0] from the previous step\n",
        "header_row = df_calibration.iloc[0]\n",
        "\n",
        "# The first column 'Test' will be a key identifier. The 'Unnamed: 1' column is empty.\n",
        "# The actual test names are from the third column onwards (index 2).\n",
        "# We will use the 'Test' column's current name for the first column, and the values from header_row for others.\n",
        "\n",
        "# Construct the new column names\n",
        "# The first column will be renamed later for clarity if needed, for now use 'Test'\n",
        "new_columns = ['Test_Type'] + list(header_row[2:].values)\n",
        "\n",
        "# Reload the data, skipping the initial 15 rows + 5 metadata rows = 20 rows\n",
        "# Use the constructed new_columns for the header\n",
        "df_calibration_cleaned = pd.read_csv(\n",
        "    '/content/manufacturing_data.CSV',\n",
        "    skiprows=20, # Skip first 15 rows + 5 metadata rows\n",
        "    header=None, # No header row in the data to be read from skiprows=20\n",
        "    names=new_columns # Assign custom column names\n",
        ")\n",
        "\n",
        "print(\"First few rows of the truly tabular DataFrame:\")\n",
        "print(df_calibration_cleaned.head())\n",
        "print(\"\\nColumn names:\")\n",
        "print(df_calibration_cleaned.columns)\n",
        "print(\"\\nData types and non-null values:\")\n",
        "df_calibration_cleaned.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2023fe12"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the generated column names contained duplicates, which is not allowed by pandas when assigning column names. To fix this, I will modify the column name generation logic to ensure all column names are unique by appending numerical suffixes to duplicate names. This will allow the data to be loaded correctly with a clean header.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "530182fe"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extract potential column names from the first meaningful row (line 16 of original CSV)\n",
        "# This is df_calibration.iloc[0] from the previous step\n",
        "header_row = df_calibration.iloc[0]\n",
        "\n",
        "# The first column 'Test' will be a key identifier. The 'Unnamed: 1' column is empty.\n",
        "# The actual test names are from the third column onwards (index 2).\n",
        "\n",
        "# Construct the new column names, ensuring uniqueness\n",
        "new_columns_raw = list(header_row[2:].values)\n",
        "\n",
        "unique_columns = []\n",
        "counts = {}\n",
        "for col in new_columns_raw:\n",
        "    # Clean whitespace from column names\n",
        "    col = str(col).strip()\n",
        "    if col in counts:\n",
        "        counts[col] += 1\n",
        "        unique_columns.append(f\"{col}_{counts[col]}\")\n",
        "    else:\n",
        "        counts[col] = 0 # Initialize count for first occurrence\n",
        "        unique_columns.append(col) # No suffix for the first occurrence\n",
        "\n",
        "# The first column will be 'Test_Type' based on previous intention.\n",
        "new_columns = ['Test_Type'] + unique_columns\n",
        "\n",
        "# Reload the data, skipping the initial 15 rows + 5 metadata rows = 20 rows\n",
        "# Use the constructed new_columns for the header\n",
        "df_calibration_cleaned = pd.read_csv(\n",
        "    '/content/manufacturing_data.CSV',\n",
        "    skiprows=20, # Skip first 15 rows + 5 metadata rows\n",
        "    header=None, # No header row in the data to be read from skiprows=20\n",
        "    names=new_columns # Assign custom column names\n",
        ")\n",
        "\n",
        "print(\"First few rows of the truly tabular DataFrame:\")\n",
        "print(df_calibration_cleaned.head())\n",
        "print(\"\\nColumn names:\")\n",
        "print(df_calibration_cleaned.columns)\n",
        "print(\"\\nData types and non-null values:\")\n",
        "df_calibration_cleaned.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f301881d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the data correctly resulted in metadata rows still being present and the first column implicitly used as the index. Based on careful inspection of the previous `df_calibration` output, the true header row for the actual calibration measurements is found on line 16 of the original CSV, and the numerical data starts on line 31. Therefore, I will first read just the header row to extract unique column names, and then reload the data skipping all preceding metadata (30 rows in total) to get a clean tabular DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a6caf8f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the header row (line 16 of the original CSV) to extract column names\n",
        "# skiprows=15 gets to line 16, nrows=1 reads only that line.\n",
        "header_df = pd.read_csv('/content/manufacturing_data.CSV', skiprows=15, nrows=1, header=None, engine='python')\n",
        "\n",
        "# Extract potential column names from header_df, starting from the third column (index 2)\n",
        "# The first column will be a test identifier, the second is often empty.\n",
        "new_columns_raw = list(header_df.iloc[0, 2:].values)\n",
        "\n",
        "# Ensure column names are unique by appending numerical suffixes\n",
        "unique_columns = []\n",
        "counts = {}\n",
        "for col in new_columns_raw:\n",
        "    col = str(col).strip()\n",
        "    if col in counts:\n",
        "        counts[col] += 1\n",
        "        unique_columns.append(f\"{col}_{counts[col]}\")\n",
        "    else:\n",
        "        counts[col] = 0 # Initialize count for first occurrence\n",
        "        unique_columns.append(col) # No suffix for the first occurrence\n",
        "\n",
        "# The final list of column names: 'Test_Type' for the first column,\n",
        "# 'Unnamed_Col' for the second (often empty), followed by the unique test names.\n",
        "new_columns = ['Test_Type', 'Unnamed_Col'] + unique_columns\n",
        "\n",
        "# Step 2: Load the actual calibration data\n",
        "# Skip 30 rows to start reading from line 31 of the original CSV (where the actual numerical data begins).\n",
        "# header=None indicates that there's no header row in the data being read at skiprows=30,\n",
        "# and names=new_columns assigns our constructed column names.\n",
        "df_calibration_cleaned = pd.read_csv(\n",
        "    '/content/manufacturing_data.CSV',\n",
        "    skiprows=30, # Skip 15 (initial metadata) + 1 (header row) + 14 (further metadata) = 30 rows\n",
        "    header=None, # No header row in the data part\n",
        "    names=new_columns # Assign the unique column names\n",
        ")\n",
        "\n",
        "print(\"First few rows of the truly tabular DataFrame:\")\n",
        "print(df_calibration_cleaned.head())\n",
        "print(\"\\nColumn names:\")\n",
        "print(df_calibration_cleaned.columns)\n",
        "print(\"\\nData types and non-null values:\")\n",
        "df_calibration_cleaned.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f33f9b40"
      },
      "source": [
        "## Calculate Descriptive Statistics\n",
        "\n",
        "### Subtask:\n",
        "Compute and display descriptive statistics (mean, standard deviation, min, max, etc.) for the numerical columns in the calibration data to get an overview of the value distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65b54225"
      },
      "source": [
        "**Reasoning**:\n",
        "To get an overview of the value distributions, I will identify all numerical columns in the `df_calibration_cleaned` DataFrame and then calculate their descriptive statistics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a05e2d66"
      },
      "source": [
        "print(\"Descriptive statistics for numerical columns:\")\n",
        "# Identify numerical columns\n",
        "numerical_columns = df_calibration_cleaned.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate descriptive statistics\n",
        "descriptive_stats = numerical_columns.describe()\n",
        "\n",
        "# Display the results\n",
        "print(descriptive_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5098b8f"
      },
      "source": [
        "## Visualize Value Distributions\n",
        "\n",
        "### Subtask:\n",
        "Generate histograms or box plots for key numerical columns to visualize the distribution of calibration values and identify any potential outliers or patterns. Include appropriate legends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f76dd71"
      },
      "source": [
        "**Reasoning**:\n",
        "To visualize the distribution of calibration values, I will generate histograms and box plots for the specified key numerical columns, ensuring each plot has appropriate labels and titles for clarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8473b42"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select a few representative numerical columns for visualization\n",
        "selected_columns = ['2', '3', '241', '243', '249']\n",
        "\n",
        "# --- Generate Histograms ---\n",
        "print(\"Generating Histograms for selected numerical columns...\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(selected_columns):\n",
        "    plt.subplot(2, 3, i + 1) # Arrange in 2 rows, 3 columns\n",
        "    sns.histplot(df_calibration_cleaned[col], kde=True)\n",
        "    plt.title(f'Histogram of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Generate Box Plots ---\n",
        "print(\"\\nGenerating Box Plots for selected numerical columns...\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(selected_columns):\n",
        "    plt.subplot(2, 3, i + 1) # Arrange in 2 rows, 3 columns\n",
        "    sns.boxplot(y=df_calibration_cleaned[col])\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "    plt.ylabel(col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47b4eec9"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the initial findings from the data inspection and descriptive statistics, highlighting any interesting observations about the calibration results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cca91e7b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The initial findings from the data inspection and descriptive statistics indicate that the manufacturing data, after extensive cleaning, shows remarkable consistency in many calibration results. Most numerical calibration parameters exhibit extremely low standard deviations (in the order of \\$10^{-10}\\$ to \\$10^{-15}\\$), suggesting a highly stable process or very tight control limits. Some columns, like 'Unnamed_Col', are constant (value 250), while 'Column 1' appears to contain non-numerical or missing data, leading to `NaN` statistics. The 'Test_Type' column has a broader distribution, possibly serving as an identifier.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The initial CSV file presented significant parsing challenges due to an inconsistent number of fields per line and embedded metadata, requiring `on_bad_lines='skip'` and subsequent careful `skiprows` and header handling.\n",
        "*   The raw data format was non-standard, containing metadata over multiple initial rows (up to row 30 of the original CSV) before the actual tabular calibration results began on line 31.\n",
        "*   The true column headers for the calibration results were found on line 16 of the original CSV, and these headers contained duplicate names (e.g., 'SAME'), necessitating a custom column naming strategy with numerical suffixes (e.g., 'SAME\\_1', 'SAME\\_2') to ensure uniqueness.\n",
        "*   After cleaning, the `df_calibration_cleaned` DataFrame consists of 250 rows and 253 columns, with most columns correctly identified as numerical (float64).\n",
        "*   Descriptive statistics reveal that 252 columns are numerical. Among these, many (e.g., columns '2' through '250') exhibit extremely small standard deviations, ranging from approximately \\$10^{-10}\\$ to \\$10^{-15}\\$. This indicates a very high degree of consistency or negligible variation in these specific calibration measurements.\n",
        "*   The 'Unnamed\\_Col' column has a standard deviation of 0, meaning all its values are identical (250).\n",
        "*   Column '1' shows `NaN` for its descriptive statistics, suggesting it might contain entirely non-numerical data or be empty.\n",
        "*   'Test\\_Type' has a standard deviation of 4.47, indicating it has more variation compared to the highly consistent calibration values and likely serves as a categorical or identifier column.\n",
        "*   Visualizations (histograms and box plots) for selected key numerical columns ('2', '3', '241', '243', '249') visually confirm their narrow distributions and tightly clustered values, reinforcing the observation of high consistency.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The observed extremely low standard deviations for numerous calibration parameters suggest a very stable manufacturing process or highly effective calibration procedures. Further investigation could involve comparing these parameters against historical data or specification limits to ensure they are consistently within acceptable ranges.\n",
        "*   The 'Column 1' column, which resulted in `NaN` descriptive statistics, requires further examination to understand its content and purpose. If it's intended to be numerical, data type conversion or imputation might be necessary; otherwise, it could be dropped if irrelevant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0458aef0"
      },
      "source": [
        "# Task\n",
        "The task is to prepare a Python function to load semiconductor calibration data from a CSV file. This function should handle complex data structures by skipping initial metadata rows, dynamically extracting unique column headers from the 16th line of the CSV, and separating calibration measurements from their corresponding limit values and limit directions (e.g., '<', '>'). The function will take the file path as input and return the cleaned measurements, limits, and limit directions. This function will be reusable for both initial and adjusted calibration files, beginning with the provided '/content/manufacturing_data.CSV' file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caf63173"
      },
      "source": [
        "## Prepare Data Loading and Limit Extraction Function\n",
        "\n",
        "### Subtask:\n",
        "Develop a robust Python function to load semiconductor calibration data from a CSV file, extracting measurements, limit values, and limit directions, and handling complex data structures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57008d2f"
      },
      "source": [
        "**Reasoning**:\n",
        "To encapsulate the data loading and parsing logic, I will define a Python function `load_calibration_data` that performs the specified steps: reading header information, generating unique column names, extracting limit values and directions, and finally loading the main measurement data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0c8533f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_calibration_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads semiconductor calibration data from a CSV file, extracting measurements,\n",
        "    limit values, and limit directions, while handling complex data structures\n",
        "    and ensuring unique column names.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CSV file containing the calibration data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - df_measurements (pd.DataFrame): DataFrame with the cleaned calibration measurements.\n",
        "            - limit_values (pd.Series): Series containing the extracted limit values,\n",
        "                                        aligned with the processed column names.\n",
        "            - limit_directions (pd.Series): Series containing the extracted limit directions,\n",
        "                                            aligned with the processed column names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1 & 2: Read the header rows containing test names, limit values, and limit directions\n",
        "    # skiprows=15 gets to line 16, nrows=3 reads lines 16, 17, 18.\n",
        "    header_info_df = pd.read_csv(file_path, skiprows=15, nrows=3, header=None, engine='python')\n",
        "\n",
        "    # Step 3: Extract raw column names from the first row of header_info_df (index 0),\n",
        "    # starting from the third column (index 2).\n",
        "    new_columns_raw = list(header_info_df.iloc[0, 2:].values)\n",
        "\n",
        "    # Step 4: Process column names to ensure uniqueness and clean them.\n",
        "    unique_columns = []\n",
        "    counts = {}\n",
        "    for col in new_columns_raw:\n",
        "        col = str(col).strip() # Clean whitespace\n",
        "        if col in counts:\n",
        "            counts[col] += 1\n",
        "            unique_columns.append(f\"{col}_{counts[col]}\")\n",
        "        else:\n",
        "            counts[col] = 0  # Initialize count for first occurrence\n",
        "            unique_columns.append(col) # No suffix for the first occurrence\n",
        "\n",
        "    # The first two columns are 'Test_Type' and 'Unnamed_Col'\n",
        "    final_column_names = ['Test_Type', 'Unnamed_Col'] + unique_columns\n",
        "\n",
        "    # Step 5: Extract limit values from the second row (index 1) of header_info_df,\n",
        "    # starting from the third column (index 2).\n",
        "    limit_values_raw = header_info_df.iloc[1, 2:].values\n",
        "    # Convert to numeric, coercing errors to NaN, and align with unique columns\n",
        "    limit_values = pd.Series(pd.to_numeric(limit_values_raw, errors='coerce'), index=unique_columns)\n",
        "\n",
        "    # Step 6: Extract limit directions from the third row (index 2) of header_info_df,\n",
        "    # starting from the third column (index 2).\n",
        "    limit_directions_raw = header_info_df.iloc[2, 2:].values\n",
        "    # Align with unique columns\n",
        "    limit_directions = pd.Series(limit_directions_raw, index=unique_columns)\n",
        "\n",
        "    # Step 7: Reload the main data, skipping initial metadata and assigning new column names.\n",
        "    # skiprows=30 gets to line 31 of the original CSV, which is where actual data starts.\n",
        "    df_measurements = pd.read_csv(\n",
        "        file_path,\n",
        "        skiprows=30,\n",
        "        header=None,\n",
        "        names=final_column_names,\n",
        "        engine='python',\n",
        "        on_bad_lines='skip' # Retain on_bad_lines='skip' from previous successful load attempts\n",
        "    )\n",
        "\n",
        "    # Step 8: Return the cleaned DataFrame, limit values, and limit directions.\n",
        "    return df_measurements, limit_values, limit_directions\n",
        "\n",
        "# Call the function to load the data\n",
        "file_path = '/content/manufacturing_data.CSV'\n",
        "df_measurements, limit_values, limit_directions = load_calibration_data(file_path)\n",
        "\n",
        "print(\"First few rows of the cleaned DataFrame (df_measurements):\")\n",
        "print(df_measurements.head())\n",
        "print(\"\\nLimit Values for a few columns:\")\n",
        "print(limit_values.head())\n",
        "print(\"\\nLimit Directions for a few columns:\")\n",
        "print(limit_directions.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85469e2a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to extract limit values and directions was incorrect because the indices for `header_info_df` were misaligned with the actual content of the header rows, and the `nrows` parameter was insufficient. I need to adjust the `nrows` to 4 and correct the row indices for extracting `unique_columns` (actual test names), `limit_values`, and `limit_directions` to ensure they are parsed from the correct lines in the CSV and consistently mapped to the measurement data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ace52754"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_calibration_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads semiconductor calibration data from a CSV file, extracting measurements,\n",
        "    limit values, and limit directions, while handling complex data structures\n",
        "    and ensuring unique column names.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CSV file containing the calibration data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - df_measurements (pd.DataFrame): DataFrame with the cleaned calibration measurements.\n",
        "            - limit_values (pd.Series): Series containing the extracted limit values,\n",
        "                                        aligned with the processed column names.\n",
        "            - limit_directions (pd.Series): Series containing the extracted limit directions,\n",
        "                                            aligned with the processed column names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1 & 2: Read the header rows containing test names, limit values, and limit directions\n",
        "    # skiprows=15 gets to line 16. We need to read 4 rows to get 'Test/Unnamed', 'Item', 'Limit', 'Limit Min Max'.\n",
        "    header_info_df = pd.read_csv(file_path, skiprows=15, nrows=4, header=None, engine='python')\n",
        "\n",
        "    # From previous inspection:\n",
        "    # header_info_df.iloc[0] contains: [Test, NaN, 1, 2, 3, ...]\n",
        "    # header_info_df.iloc[1] contains: [Item, NaN, CONT, IGSS, SAME, ...] <- These are the actual test names\n",
        "    # header_info_df.iloc[2] contains: [Limit, NaN, 0.0, 2.537e-07, 2.462e-007, ...] <- These are the limit values\n",
        "    # header_info_df.iloc[3] contains: [Limit Min Max, NaN, '', <, >, ...] <- These are the limit directions\n",
        "\n",
        "    # Step 3: Extract raw column names from the 'Item' row (index 1) of header_info_df,\n",
        "    # starting from the third column (index 2).\n",
        "    actual_test_names_raw = list(header_info_df.iloc[1, 2:].values)\n",
        "\n",
        "    # Step 4: Process column names to ensure uniqueness and clean them.\n",
        "    unique_columns = []\n",
        "    counts = {}\n",
        "    for col in actual_test_names_raw:\n",
        "        col = str(col).strip() # Clean whitespace\n",
        "        if col in counts:\n",
        "            counts[col] += 1\n",
        "            unique_columns.append(f\"{col}_{counts[col]}\")\n",
        "        else:\n",
        "            counts[col] = 0  # Initialize count for first occurrence\n",
        "            unique_columns.append(col) # No suffix for the first occurrence\n",
        "\n",
        "    # The first two columns of the data are 'Test_Type' and 'Unnamed_Col'\n",
        "    final_column_names = ['Test_Type', 'Unnamed_Col'] + unique_columns\n",
        "\n",
        "    # Step 5: Extract limit values from the 'Limit' row (index 2) of header_info_df,\n",
        "    # starting from the third column (index 2).\n",
        "    limit_values_raw = header_info_df.iloc[2, 2:].values\n",
        "    # Convert to numeric, coercing errors to NaN, and align with unique columns\n",
        "    limit_values = pd.Series(pd.to_numeric(limit_values_raw, errors='coerce'), index=unique_columns)\n",
        "\n",
        "    # Step 6: Extract limit directions from the 'Limit Min Max' row (index 3) of header_info_df,\n",
        "    # starting from the third column (index 2).\n",
        "    limit_directions_raw = header_info_df.iloc[3, 2:].values\n",
        "    # Align with unique columns\n",
        "    limit_directions = pd.Series(limit_directions_raw, index=unique_columns)\n",
        "\n",
        "    # Step 7: Reload the main data, skipping initial metadata and assigning new column names.\n",
        "    # skiprows=15 (initial metadata) + 4 (header rows read above) + 11 (additional metadata rows from 'Limit Units' to 'Bias 6 Value') = 30 rows\n",
        "    # This is consistent with the previous successful df_calibration_cleaned loading.\n",
        "    df_measurements = pd.read_csv(\n",
        "        file_path,\n",
        "        skiprows=30,\n",
        "        header=None,\n",
        "        names=final_column_names,\n",
        "        engine='python',\n",
        "        on_bad_lines='skip' # Retain on_bad_lines='skip' from previous successful load attempts\n",
        "    )\n",
        "\n",
        "    # Step 8: Return the cleaned DataFrame, limit values, and limit directions.\n",
        "    return df_measurements, limit_values, limit_directions\n",
        "\n",
        "# Call the function to load the data\n",
        "file_path = '/content/manufacturing_data.CSV'\n",
        "df_measurements, limit_values, limit_directions = load_calibration_data(file_path)\n",
        "\n",
        "print(\"First few rows of the cleaned DataFrame (df_measurements):\")\n",
        "print(df_measurements.head())\n",
        "print(\"\\nLimit Values for a few columns:\")\n",
        "print(limit_values.head())\n",
        "print(\"\\nLimit Directions for a few columns:\")\n",
        "print(limit_directions.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c256779"
      },
      "source": [
        "## Load and Process Initial Calibration Data\n",
        "\n",
        "### Subtask:\n",
        "Use the prepared data loading function to load the initial calibration data from '/content/manufacturing_data.CSV'. This step will extract the measurements and their corresponding limits, and prepare the data for analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b22502be"
      },
      "source": [
        "**Reasoning**:\n",
        "To load the initial calibration data and extract measurements, limit values, and limit directions, I will call the previously defined `load_calibration_data` function with the specified file path and store its outputs in the designated variables. Then, I will print the heads of the resulting DataFrames and Series to verify the successful loading and extraction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8935c71d"
      },
      "source": [
        "file_path = '/content/manufacturing_data.CSV'\n",
        "df_initial_measurements, initial_limit_values, initial_limit_directions = load_calibration_data(file_path)\n",
        "\n",
        "print(\"First few rows of df_initial_measurements:\")\n",
        "print(df_initial_measurements.head())\n",
        "\n",
        "print(\"\\nFirst few initial_limit_values:\")\n",
        "print(initial_limit_values.head())\n",
        "\n",
        "print(\"\\nFirst few initial_limit_directions:\")\n",
        "print(initial_limit_directions.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44fc92e6"
      },
      "source": [
        "## Analyze Initial Calibration for Tolerances\n",
        "\n",
        "### Subtask:\n",
        "Develop a function to evaluate each measurement in the initial calibration against its defined limits and a 1.5% error tolerance, categorizing tests as 'stable', 'close to limit', or 'out of tolerance'. Additionally, identify and list the 10 most critical tests based on their deviation from limits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8018d215"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define a Python function `analyze_calibration_tolerances` that takes the cleaned measurement DataFrame, limit values, limit directions, and a tolerance percentage as input. This function will evaluate each measurement against its limits and categorize them as 'stable', 'close to limit', or 'out of tolerance', and also identify the most critical tests.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efc7105d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def analyze_calibration_tolerances(df_measurements, limit_values, limit_directions, tolerance_percentage):\n",
        "    \"\"\"\n",
        "    Evaluates each measurement in the initial calibration against its defined limits and\n",
        "    a given error tolerance, categorizing tests as 'stable', 'close to limit', or 'out of tolerance'.\n",
        "    Identifies the 10 most critical tests based on their deviation from limits.\n",
        "\n",
        "    Args:\n",
        "        df_measurements (pd.DataFrame): DataFrame with the cleaned calibration measurements.\n",
        "        limit_values (pd.Series): Series containing the extracted limit values, aligned with column names.\n",
        "        limit_directions (pd.Series): Series containing the extracted limit directions, aligned with column names.\n",
        "        tolerance_percentage (float): The percentage tolerance to define 'close to limit' vs 'out of tolerance'.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - overall_calibration_status (str): Overall status of the calibration ('stable', 'close to limit', 'out of tolerance').\n",
        "            - aggregated_status_counts (dict): Total counts for each category across all measurements.\n",
        "            - critical_tests (list): A list of the 10 most critical tests based on maximum deviation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Identify calibration test columns (excluding 'Test_Type' and 'Unnamed_Col')\n",
        "    calibration_columns = [col for col in df_measurements.columns if col not in ['Test_Type', 'Unnamed_Col']]\n",
        "\n",
        "    # Initialize data structures\n",
        "    test_status_counts = {col: {'stable': 0, 'close to limit': 0, 'out of tolerance': 0} for col in calibration_columns}\n",
        "    max_deviations = {col: 0.0 for col in calibration_columns} # Store max absolute percentage deviation\n",
        "\n",
        "    # Overall aggregation flags\n",
        "    overall_out_of_tolerance_flag = False\n",
        "    overall_close_to_limit_flag = False\n",
        "\n",
        "    # Iterate through each calibration test column\n",
        "    for col in calibration_columns:\n",
        "        limit_val = limit_values.get(col)\n",
        "        limit_dir = limit_directions.get(col)\n",
        "\n",
        "        if pd.isna(limit_val): # Skip if no limit defined for the test\n",
        "            continue\n",
        "\n",
        "        # Iterate through each individual measurement in the current test column\n",
        "        for measurement in df_measurements[col].dropna():\n",
        "            status = ''\n",
        "            deviation_percentage = 0.0\n",
        "\n",
        "            # Case 1: Limit value is zero\n",
        "            if limit_val == 0:\n",
        "                if measurement == 0:\n",
        "                    status = 'stable'\n",
        "                else:\n",
        "                    status = 'out of tolerance'\n",
        "                    deviation_percentage = np.inf # Assign high deviation for ranking\n",
        "            # Case 2: Limit value is non-zero\n",
        "            else:\n",
        "                abs_deviation = abs(measurement - limit_val)\n",
        "                deviation_percentage = (abs_deviation / abs(limit_val)) * 100\n",
        "\n",
        "                if limit_dir == '<': # Upper limit\n",
        "                    if measurement <= limit_val:\n",
        "                        status = 'stable'\n",
        "                    elif deviation_percentage > tolerance_percentage:\n",
        "                        status = 'out of tolerance'\n",
        "                    else:\n",
        "                        status = 'close to limit'\n",
        "                elif limit_dir == '>': # Lower limit\n",
        "                    if measurement >= limit_val:\n",
        "                        status = 'stable'\n",
        "                    elif deviation_percentage > tolerance_percentage:\n",
        "                        status = 'out of tolerance'\n",
        "                    else:\n",
        "                        status = 'close to limit'\n",
        "                elif limit_dir == '': # Exact or nominal limit\n",
        "                    if measurement == limit_val:\n",
        "                        status = 'stable'\n",
        "                    elif deviation_percentage > tolerance_percentage:\n",
        "                        status = 'out of tolerance'\n",
        "                    else:\n",
        "                        status = 'close to limit'\n",
        "                else: # Default if direction is unrecognized or missing, treat as exact\n",
        "                    if measurement == limit_val:\n",
        "                        status = 'stable'\n",
        "                    elif deviation_percentage > tolerance_percentage:\n",
        "                        status = 'out of tolerance'\n",
        "                    else:\n",
        "                        status = 'close to limit'\n",
        "\n",
        "            # Update counts for the current test\n",
        "            if status:\n",
        "                test_status_counts[col][status] += 1\n",
        "\n",
        "            # Update max deviation for the current test\n",
        "            max_deviations[col] = max(max_deviations[col], deviation_percentage)\n",
        "\n",
        "            # Update overall flags\n",
        "            if status == 'out of tolerance':\n",
        "                overall_out_of_tolerance_flag = True\n",
        "            elif status == 'close to limit':\n",
        "                overall_close_to_limit_flag = True\n",
        "\n",
        "    # Aggregate overall status counts\n",
        "    aggregated_status_counts = {'stable': 0, 'close to limit': 0, 'out of tolerance': 0}\n",
        "    for col in calibration_columns:\n",
        "        for status_type, count in test_status_counts[col].items():\n",
        "            aggregated_status_counts[status_type] += count\n",
        "\n",
        "    # Determine overall calibration status\n",
        "    if overall_out_of_tolerance_flag:\n",
        "        overall_calibration_status = 'out of tolerance'\n",
        "    elif overall_close_to_limit_flag:\n",
        "        overall_calibration_status = 'close to limit'\n",
        "    else:\n",
        "        overall_calibration_status = 'stable'\n",
        "\n",
        "    # Identify the 10 most critical tests\n",
        "    # Filter out tests with 0 deviation (perfectly stable or no data evaluated)\n",
        "    criticality_scores = {test: dev for test, dev in max_deviations.items() if dev > 0 or dev == np.inf}\n",
        "    critical_tests = sorted(criticality_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "    critical_tests = [test for test, _ in critical_tests[:10]] # Get top 10 names\n",
        "\n",
        "    return overall_calibration_status, aggregated_status_counts, critical_tests\n",
        "\n",
        "# Example usage with the loaded data:\n",
        "tolerance_percent = 1.5\n",
        "overall_status, status_counts, critical_tests = analyze_calibration_tolerances(\n",
        "    df_initial_measurements, initial_limit_values, initial_limit_directions, tolerance_percent\n",
        ")\n",
        "\n",
        "print(f\"Overall Calibration Status: {overall_status}\")\n",
        "print(f\"Aggregated Status Counts: {status_counts}\")\n",
        "print(f\"Top 10 Critical Tests: {critical_tests}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa6ca156"
      },
      "source": [
        "## Generate Initial Calibration Report\n",
        "\n",
        "### Subtask:\n",
        "Based on the analysis, generate a summary report for the initial calibration. This report will include the overall calibration status (e.g., 'stable', 'close to limits', 'out of tolerance'), a count of tests in each category, and a list of the 10 most critical tests. The report will be saved to a uniquely named file (including timestamp and engineer's name) and will include a suggestion for further action if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ff2da7c"
      },
      "source": [
        "**Reasoning**:\n",
        "To generate the summary report, I will import the `datetime` module, define the engineer's name, construct the report string using the analysis results, create a unique filename with a timestamp and engineer's name, and then save the report to that file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3933c77"
      },
      "source": [
        "import datetime\n",
        "\n",
        "# 2. Define a variable engineer_name\n",
        "engineer_name = 'AI Agent'\n",
        "\n",
        "# Get current timestamp for unique filename\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "# 3. Construct a detailed report string\n",
        "report_content = f\"Calibration Report - {timestamp} by {engineer_name}\\n\\n\"\n",
        "report_content += \"-----------------------------------------------------\\n\"\n",
        "report_content += f\"Overall Calibration Status: {overall_status}\\n\"\n",
        "report_content += \"-----------------------------------------------------\\n\"\n",
        "report_content += \"Aggregated Test Status Counts:\\n\"\n",
        "for status_type, count in status_counts.items():\n",
        "    report_content += f\"  - {status_type.replace('_', ' ').title()}: {count}\\n\"\n",
        "report_content += \"\\n\"\n",
        "report_content += \"Top 10 Critical Tests (based on maximum deviation):\\n\"\n",
        "if critical_tests:\n",
        "    for i, test in enumerate(critical_tests):\n",
        "        report_content += f\"  {i+1}. {test}\\n\"\n",
        "else:\n",
        "    report_content += \"  No critical tests identified within the given tolerance.\\n\"\n",
        "\n",
        "report_content += \"\\n\"\n",
        "report_content += \"Suggestions for Further Action:\\n\"\n",
        "if overall_status == 'out of tolerance':\n",
        "    report_content += \"  Immediate recalibration is strongly recommended for the identified out-of-tolerance tests. Further investigation into the root cause of these deviations is crucial.\"\n",
        "elif overall_status == 'close to limit':\n",
        "    report_content += \"  Close monitoring of the identified tests is advised. Minor adjustments to the calibration process or equipment might be necessary to prevent future out-of-tolerance conditions.\"\n",
        "elif overall_status == 'stable':\n",
        "    report_content += \"  The calibration results are stable. Continue with routine checks and scheduled maintenance to maintain this performance. Consider reviewing the tolerance limits if all tests are consistently far from limits.\"\n",
        "\n",
        "# 4. Create a unique filename\n",
        "report_filename = f\"calibration_report_{timestamp}_{engineer_name.replace(' ', '_')}.txt\"\n",
        "\n",
        "# 5. Save the constructed report string to the file\n",
        "with open(report_filename, 'w') as f:\n",
        "    f.write(report_content)\n",
        "\n",
        "print(f\"Calibration report saved to: {report_filename}\")\n",
        "print(\"\\nReport Content Preview:\\n\")\n",
        "print(report_content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67de17db"
      },
      "source": [
        "## Load and Process Adjusted Calibration Data\n",
        "\n",
        "### Subtask:\n",
        "Prompt the user for the adjusted calibration data file path and load the data using the `load_calibration_data` function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "655e7a46"
      },
      "source": [
        "**Reasoning**:\n",
        "To load the adjusted calibration data, I need to prompt the user for the file path, then call the `load_calibration_data` function with this path, and finally print the heads of the returned data structures to verify successful loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03cc36f8"
      },
      "source": [
        "adjusted_file_path = input(\"/content//content/\")\n",
        "\n",
        "df_adjusted_measurements, adjusted_limit_values, adjusted_limit_directions = load_calibration_data(adjusted_file_path)\n",
        "\n",
        "print(\"\\nFirst few rows of df_adjusted_measurements:\")\n",
        "print(df_adjusted_measurements.head())\n",
        "\n",
        "print(\"\\nFirst few adjusted_limit_values:\")\n",
        "print(adjusted_limit_values.head())\n",
        "\n",
        "print(\"\\nFirst few adjusted_limit_directions:\")\n",
        "print(adjusted_limit_directions.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xS6TTo4h-SFG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87e5114e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `FileNotFoundError` because the `input()` function received an empty string. To prevent this, I will assign a default file path to `adjusted_file_path` (using the existing `/content/manufacturing_data.CSV` as an example) and add a comment for the user to change it if they have a different adjusted file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58e023e0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# User should replace this with the actual path to their adjusted calibration data file.\n",
        "# For demonstration, using the original file path.\n",
        "adjusted_file_path = '/content//content/711_FT_NGloba_11JUN09.CSV'\n",
        "\n",
        "df_adjusted_measurements, adjusted_limit_values, adjusted_limit_directions = load_calibration_data(adjusted_file_path)\n",
        "\n",
        "print(\"\\nFirst few rows of df_adjusted_measurements:\")\n",
        "print(df_adjusted_measurements.head())\n",
        "\n",
        "print(\"\\nFirst few adjusted_limit_values:\")\n",
        "print(adjusted_limit_values.head())\n",
        "\n",
        "print(\"\\nFirst few adjusted_limit_directions:\")\n",
        "print(adjusted_limit_directions.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81746d58"
      },
      "source": [
        "## Analyze Adjusted Calibration for Tolerances\n",
        "\n",
        "### Subtask:\n",
        "Apply the same tolerance analysis function to the adjusted calibration data. This will provide a similar breakdown of test categories and identify critical tests for the post-adjustment scenario.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e8c3040"
      },
      "source": [
        "**Reasoning**:\n",
        "To analyze the adjusted calibration data, I will call the `analyze_calibration_tolerances` function with the adjusted measurements, limit values, and directions, and then print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e142008b"
      },
      "source": [
        "overall_adjusted_status, adjusted_status_counts, adjusted_critical_tests = analyze_calibration_tolerances(\n",
        "    df_adjusted_measurements, adjusted_limit_values, adjusted_limit_directions, tolerance_percent\n",
        ")\n",
        "\n",
        "print(f\"Overall Adjusted Calibration Status: {overall_adjusted_status}\")\n",
        "print(f\"Aggregated Adjusted Status Counts: {adjusted_status_counts}\")\n",
        "print(f\"Top 10 Critical Tests (Adjusted Data): {adjusted_critical_tests}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbaab6bb"
      },
      "source": [
        "## Perform Comparative Analysis\n",
        "\n",
        "### Subtask:\n",
        "Compare the results of the initial and adjusted calibrations. This step will quantify the shift in measurements, compare the number of 'out of tolerance' tests between both datasets, and determine if the adjustments improved the calibration performance (e.g., reduced error, brought more tests within tolerance). A key focus will be on the overall improvement in error percentage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cda59801"
      },
      "source": [
        "**Reasoning**:\n",
        "To compare the initial and adjusted calibration results as per the subtask instructions, I will define a Python function `perform_comparative_analysis` that takes the required dataframes and status counts. This function will calculate the average shift in measurements, quantify the change in 'out of tolerance' and 'close to limit' tests, and provide an overall assessment of calibration improvement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ac00421"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def perform_comparative_analysis(\n",
        "    df_initial_measurements,\n",
        "    df_adjusted_measurements,\n",
        "    initial_status_counts,\n",
        "    adjusted_status_counts,\n",
        "    initial_critical_tests,\n",
        "    adjusted_critical_tests\n",
        "):\n",
        "    \"\"\"\n",
        "    Compares the results of initial and adjusted calibrations to quantify shifts,\n",
        "    changes in 'out of tolerance' tests, and overall improvement.\n",
        "\n",
        "    Args:\n",
        "        df_initial_measurements (pd.DataFrame): DataFrame with initial calibration measurements.\n",
        "        df_adjusted_measurements (pd.DataFrame): DataFrame with adjusted calibration measurements.\n",
        "        initial_status_counts (dict): Aggregated status counts for initial calibration.\n",
        "        adjusted_status_counts (dict): Aggregated status counts for adjusted calibration.\n",
        "        initial_critical_tests (list): List of critical tests for initial calibration.\n",
        "        adjusted_critical_tests (list): List of critical tests for adjusted calibration.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing comparative results.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Identify common numerical columns\n",
        "    non_calibration_cols = ['Test_Type', 'Unnamed_Col']\n",
        "    initial_cal_cols = [col for col in df_initial_measurements.columns if col not in non_calibration_cols]\n",
        "    adjusted_cal_cols = [col for col in df_adjusted_measurements.columns if col not in non_calibration_cols]\n",
        "\n",
        "    # Ensure both dataframes have the same set of calibration columns for comparison\n",
        "    common_calibration_columns = list(set(initial_cal_cols) & set(adjusted_cal_cols))\n",
        "    common_calibration_columns.sort(key=lambda x: str(x)) # Sort for consistent order\n",
        "\n",
        "    # 2. Calculate average absolute difference (measurement shift)\n",
        "    diffs = (df_initial_measurements[common_calibration_columns] - df_adjusted_measurements[common_calibration_columns]).abs()\n",
        "    avg_measurement_shift = diffs.mean().mean() # Mean of means to get overall average shift\n",
        "\n",
        "    # 3. Compare 'out of tolerance' tests\n",
        "    initial_oot = initial_status_counts.get('out of tolerance', 0)\n",
        "    adjusted_oot = adjusted_status_counts.get('out of tolerance', 0)\n",
        "    oot_change = initial_oot - adjusted_oot\n",
        "\n",
        "    # 4. Compare 'close to limit' tests\n",
        "    initial_ctl = initial_status_counts.get('close to limit', 0)\n",
        "    adjusted_ctl = adjusted_status_counts.get('close to limit', 0)\n",
        "    ctl_change = initial_ctl - adjusted_ctl\n",
        "\n",
        "    # 5. Determine overall improvement\n",
        "    overall_assessment = \"\"\n",
        "    if oot_change > 0 and adjusted_status_counts.get('stable', 0) > initial_status_counts.get('stable', 0):\n",
        "        overall_assessment = \"Significant improvement: 'Out of tolerance' tests decreased, and 'stable' tests increased.\"\n",
        "    elif oot_change > 0:\n",
        "        overall_assessment = \"Some improvement: 'Out of tolerance' tests decreased.\"\n",
        "    elif oot_change < 0:\n",
        "        overall_assessment = \"Degradation: 'Out of tolerance' tests increased.\"\n",
        "    else:\n",
        "        overall_assessment = \"No significant change in 'out of tolerance' tests. Further analysis needed.\"\n",
        "\n",
        "    # Also consider the reduction in critical tests\n",
        "    num_initial_critical = len(initial_critical_tests)\n",
        "    num_adjusted_critical = len(adjusted_critical_tests)\n",
        "    critical_tests_change = num_initial_critical - num_adjusted_critical\n",
        "    if critical_tests_change > 0:\n",
        "        overall_assessment += f\" Number of critical tests reduced by {critical_tests_change}.\"\n",
        "    elif critical_tests_change < 0:\n",
        "        overall_assessment += f\" Number of critical tests increased by {abs(critical_tests_change)}.\"\n",
        "\n",
        "\n",
        "    return {\n",
        "        'avg_measurement_shift': avg_measurement_shift,\n",
        "        'out_of_tolerance_change': oot_change,\n",
        "        'close_to_limit_change': ctl_change,\n",
        "        'initial_out_of_tolerance_count': initial_oot,\n",
        "        'adjusted_out_of_tolerance_count': adjusted_oot,\n",
        "        'initial_stable_count': initial_status_counts.get('stable', 0),\n",
        "        'adjusted_stable_count': adjusted_status_counts.get('stable', 0),\n",
        "        'overall_assessment': overall_assessment,\n",
        "        'initial_critical_tests_count': num_initial_critical,\n",
        "        'adjusted_critical_tests_count': num_adjusted_critical\n",
        "    }\n",
        "\n",
        "# Call the function with the relevant initial and adjusted data\n",
        "comparative_results = perform_comparative_analysis(\n",
        "    df_initial_measurements,\n",
        "    df_adjusted_measurements,\n",
        "    initial_status_counts,\n",
        "    adjusted_status_counts,\n",
        "    initial_critical_tests,\n",
        "    adjusted_critical_tests\n",
        ")\n",
        "\n",
        "print(\"\\n--- Comparative Analysis Results ---\")\n",
        "for key, value in comparative_results.items():\n",
        "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f809d0f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the variables `initial_status_counts` and `initial_critical_tests` were not explicitly defined before being passed to the `perform_comparative_analysis` function. Based on the previous execution, `status_counts` and `critical_tests` hold the required data for the initial calibration. I will correct the variable names in the function call to use the existing variables from the kernel state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b828814"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def perform_comparative_analysis(\n",
        "    df_initial_measurements,\n",
        "    df_adjusted_measurements,\n",
        "    initial_status_counts,\n",
        "    adjusted_status_counts,\n",
        "    initial_critical_tests,\n",
        "    adjusted_critical_tests\n",
        "):\n",
        "    \"\"\"\n",
        "    Compares the results of initial and adjusted calibrations to quantify shifts,\n",
        "    changes in 'out of tolerance' tests, and overall improvement.\n",
        "\n",
        "    Args:\n",
        "        df_initial_measurements (pd.DataFrame): DataFrame with initial calibration measurements.\n",
        "        df_adjusted_measurements (pd.DataFrame): DataFrame with adjusted calibration measurements.\n",
        "        initial_status_counts (dict): Aggregated status counts for initial calibration.\n",
        "        adjusted_status_counts (dict): Aggregated status counts for adjusted calibration.\n",
        "        initial_critical_tests (list): List of critical tests for initial calibration.\n",
        "        adjusted_critical_tests (list): List of critical tests for adjusted calibration.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing comparative results.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Identify common numerical columns\n",
        "    non_calibration_cols = ['Test_Type', 'Unnamed_Col']\n",
        "    initial_cal_cols = [col for col in df_initial_measurements.columns if col not in non_calibration_cols]\n",
        "    adjusted_cal_cols = [col for col in df_adjusted_measurements.columns if col not in non_calibration_cols]\n",
        "\n",
        "    # Ensure both dataframes have the same set of calibration columns for comparison\n",
        "    common_calibration_columns = list(set(initial_cal_cols) & set(adjusted_cal_cols))\n",
        "    common_calibration_columns.sort(key=lambda x: str(x)) # Sort for consistent order\n",
        "\n",
        "    # 2. Calculate average absolute difference (measurement shift)\n",
        "    diffs = (df_initial_measurements[common_calibration_columns] - df_adjusted_measurements[common_calibration_columns]).abs()\n",
        "    avg_measurement_shift = diffs.mean().mean() # Mean of means to get overall average shift\n",
        "\n",
        "    # 3. Compare 'out of tolerance' tests\n",
        "    initial_oot = initial_status_counts.get('out of tolerance', 0)\n",
        "    adjusted_oot = adjusted_status_counts.get('out of tolerance', 0)\n",
        "    oot_change = initial_oot - adjusted_oot\n",
        "\n",
        "    # 4. Compare 'close to limit' tests\n",
        "    initial_ctl = initial_status_counts.get('close to limit', 0)\n",
        "    adjusted_ctl = adjusted_status_counts.get('close to limit', 0)\n",
        "    ctl_change = initial_ctl - adjusted_ctl\n",
        "\n",
        "    # 5. Determine overall improvement\n",
        "    overall_assessment = \"\"\n",
        "    if oot_change > 0 and adjusted_status_counts.get('stable', 0) > initial_status_counts.get('stable', 0):\n",
        "        overall_assessment = \"Significant improvement: 'Out of tolerance' tests decreased, and 'stable' tests increased.\"\n",
        "    elif oot_change > 0:\n",
        "        overall_assessment = \"Some improvement: 'Out of tolerance' tests decreased.\"\n",
        "    elif oot_change < 0:\n",
        "        overall_assessment = \"Degradation: 'Out of tolerance' tests increased.\"\n",
        "    else:\n",
        "        overall_assessment = \"No significant change in 'out of tolerance' tests. Further analysis needed.\"\n",
        "\n",
        "    # Also consider the reduction in critical tests\n",
        "    num_initial_critical = len(initial_critical_tests)\n",
        "    num_adjusted_critical = len(adjusted_critical_tests)\n",
        "    critical_tests_change = num_initial_critical - num_adjusted_critical\n",
        "    if critical_tests_change > 0:\n",
        "        overall_assessment += f\" Number of critical tests reduced by {critical_tests_change}.\"\n",
        "    elif critical_tests_change < 0:\n",
        "        overall_assessment += f\" Number of critical tests increased by {abs(critical_tests_change)}.\"\n",
        "\n",
        "\n",
        "    return {\n",
        "        'avg_measurement_shift': avg_measurement_shift,\n",
        "        'out_of_tolerance_change': oot_change,\n",
        "        'close_to_limit_change': ctl_change,\n",
        "        'initial_out_of_tolerance_count': initial_oot,\n",
        "        'adjusted_out_of_tolerance_count': adjusted_oot,\n",
        "        'initial_stable_count': initial_status_counts.get('stable', 0),\n",
        "        'adjusted_stable_count': adjusted_status_counts.get('stable', 0),\n",
        "        'overall_assessment': overall_assessment,\n",
        "        'initial_critical_tests_count': num_initial_critical,\n",
        "        'adjusted_critical_tests_count': num_adjusted_critical\n",
        "    }\n",
        "\n",
        "# Call the function with the relevant initial and adjusted data\n",
        "comparative_results = perform_comparative_analysis(\n",
        "    df_initial_measurements,\n",
        "    df_adjusted_measurements,\n",
        "    status_counts, # Corrected variable name\n",
        "    adjusted_status_counts,\n",
        "    critical_tests, # Corrected variable name\n",
        "    adjusted_critical_tests\n",
        ")\n",
        "\n",
        "print(\"\\n--- Comparative Analysis Results ---\")\n",
        "for key, value in comparative_results.items():\n",
        "    print(f\"{key.replace('_', ' ').title()}: {value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ef7f8be"
      },
      "source": [
        "## Visualize Critical Test Comparisons\n",
        "\n",
        "### Subtask:\n",
        "Generate comparative visualizations (e.g., side-by-side box plots or scatter plots) for the 10 most critical tests identified. This will visually highlight any patterns, shifts, or improvements between the initial and adjusted calibration states, including appropriate legends for clarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d152ecb3"
      },
      "source": [
        "**Reasoning**:\n",
        "To generate comparative visualizations for the 10 most critical tests, I will iterate through the `critical_tests` list and create side-by-side box plots for each test, comparing initial and adjusted calibration measurements. I will also add appropriate titles and labels for clarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6def1cab"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure `critical_tests` and dataframes are available\n",
        "# `critical_tests` comes from the `analyze_calibration_tolerances` function call on initial data\n",
        "# `df_initial_measurements` and `df_adjusted_measurements` are loaded earlier\n",
        "\n",
        "print(\"Generating comparative box plots for the 10 most critical tests...\")\n",
        "\n",
        "for test_name in critical_tests:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
        "\n",
        "    # Box plot for Initial Calibration\n",
        "    sns.boxplot(y=df_initial_measurements[test_name], ax=axes[0])\n",
        "    axes[0].set_title(f'Initial Calibration: {test_name}')\n",
        "    axes[0].set_ylabel('Measurement Value')\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Box plot for Adjusted Calibration\n",
        "    sns.boxplot(y=df_adjusted_measurements[test_name], ax=axes[1])\n",
        "    axes[1].set_title(f'Adjusted Calibration: {test_name}')\n",
        "    axes[1].set_ylabel('Measurement Value')\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9e84f32"
      },
      "source": [
        "### Observation on Comparative Visualizations\n",
        "\n",
        "It is important to note that the comparative box plots generated for the 'Initial Calibration' and 'Adjusted Calibration' currently appear identical. This is because, for demonstration purposes, the same `/content/manufacturing_data.CSV` file was used for both the initial and adjusted calibration data (`df_initial_measurements` and `df_adjusted_measurements`).\n",
        "\n",
        "In a real-world scenario, you would replace the placeholder for `adjusted_file_path` with an actual file containing the measurements after an adjustment or recalibration. This would then show meaningful differences and improvements (or degradations) in the adjusted data compared to the initial state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ae2861"
      },
      "source": [
        "### Observation on Comparative Visualizations\n",
        "\n",
        "It is important to note that the comparative box plots generated for the 'Initial Calibration' and 'Adjusted Calibration' currently appear identical. This is because, for demonstration purposes, the same `/content/manufacturing_data.CSV` file was used for both the initial and adjusted calibration data (`df_initial_measurements` and `df_adjusted_measurements`).\n",
        "\n",
        "In a real-world scenario, you would replace the placeholder for `adjusted_file_path` with an actual file containing the measurements after an adjustment or recalibration. This would then show meaningful differences and improvements (or degradations) in the adjusted data compared to the initial state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8815d938"
      },
      "source": [
        "## Generate Final Comparative Report\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings from the comparative analysis and visualizations. This report will clearly state whether the machine is now considered 'in calibration' based on the improved tolerance and error margins (specifically, if tests are stable and overall error is less than 1.5%), or if further calibration is suggested. It will highlight key patterns and insights gained from the comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3668c13f"
      },
      "source": [
        "**Reasoning**:\n",
        "To generate the final comparative report, I will determine the overall calibration status based on the adjusted data and comparative analysis, then construct a detailed report string including a summary of findings, critical tests, and recommendations, and finally save it to a unique file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fb22747"
      },
      "source": [
        "import datetime\n",
        "\n",
        "# 1. Determine the overall calibration status after adjustments\n",
        "# The machine is considered 'in calibration' if there are no 'out of tolerance' tests\n",
        "# and a high percentage of 'stable' tests, combined with a positive overall assessment.\n",
        "\n",
        "is_in_calibration = False\n",
        "if adjusted_status_counts.get('out of tolerance', 0) == 0 and\\\n",
        "   adjusted_status_counts.get('stable', 0) > (adjusted_status_counts.get('stable', 0) + adjusted_status_counts.get('close to limit', 0) + adjusted_status_counts.get('out of tolerance', 0)) * 0.95 and\\\n",
        "   \"Significant improvement\" in comparative_results['overall_assessment']:\n",
        "    is_in_calibration = True\n",
        "\n",
        "final_machine_status = \"in calibration\" if is_in_calibration else \"further calibration is suggested\"\n",
        "\n",
        "# 2. Define engineer_name (already defined in kernel, re-declaring for clarity within this step)\n",
        "engineer_name = 'AI Agent'\n",
        "\n",
        "# Get current timestamp for unique filename\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "# Construct a detailed report string\n",
        "report_content = f\"Calibration Comparative Report - {timestamp} by {engineer_name}\\n\\n\"\n",
        "report_content += \"-----------------------------------------------------\\n\"\n",
        "report_content += f\"Overall Machine Status After Adjustments: {final_machine_status.upper()}\\n\"\n",
        "report_content += \"-----------------------------------------------------\\n\\n\"\n",
        "\n",
        "report_content += \"### Summary of Comparative Analysis:\\n\"\n",
        "report_content += f\"  - Average Absolute Measurement Shift: {comparative_results['avg_measurement_shift']:.4e}\\n\"\n",
        "report_content += f\"  - Change in 'Out of Tolerance' Tests (Initial - Adjusted): {comparative_results['out_of_tolerance_change']}\\n\"\n",
        "report_content += f\"  - Change in 'Close to Limit' Tests (Initial - Adjusted): {comparative_results['close_to_limit_change']}\\n\"\n",
        "report_content += f\"  - Initial 'Out of Tolerance' Count: {comparative_results['initial_out_of_tolerance_count']}\\n\"\n",
        "report_content += f\"  - Adjusted 'Out of Tolerance' Count: {comparative_results['adjusted_out_of_tolerance_count']}\\n\"\n",
        "report_content += f\"  - Initial 'Stable' Count: {comparative_results['initial_stable_count']}\\n\"\n",
        "report_content += f\"  - Adjusted 'Stable' Count: {comparative_results['adjusted_stable_count']}\\n\"\n",
        "report_content += f\"  - Overall Assessment: {comparative_results['overall_assessment']}\\n\\n\"\n",
        "\n",
        "report_content += \"### Critical Tests Comparison:\\n\"\n",
        "report_content += f\"  - Number of Initial Critical Tests: {comparative_results['initial_critical_tests_count']}\\n\"\n",
        "report_content += f\"  - Number of Adjusted Critical Tests: {comparative_results['adjusted_critical_tests_count']}\\n\"\n",
        "report_content += f\"  - Initial Top 10 Critical Tests: {critical_tests}\\n\"\n",
        "report_content += f\"  - Adjusted Top 10 Critical Tests: {adjusted_critical_tests}\\n\\n\"\n",
        "\n",
        "report_content += f\"### Final Calibration Status: The machine is {final_machine_status}.\\n\\n\"\n",
        "\n",
        "report_content += \"### Reasoning and Insights:\\n\"\n",
        "if is_in_calibration:\n",
        "    report_content += \"  The machine is now considered 'in calibration'. The adjustments have successfully brought all tests within the acceptable limits, and a high percentage of tests are stable. The comparative analysis indicates significant improvement, confirming the effectiveness of the calibration efforts. Visualizations of critical tests also show clear shifts towards stability and within tolerance levels.\\n\"\n",
        "    report_content += \"  The error margins have been reduced effectively, aligning the machine's performance with the required standards.\"\n",
        "else:\n",
        "    report_content += \"  Based on the analysis, further calibration is suggested. The adjusted calibration data shows that there are still \"\n",
        "    report_content += f\"{adjusted_status_counts.get('out of tolerance', 0)} tests \"\n",
        "    report_content += \"that are 'out of tolerance'. The average measurement shift was minimal (0.0), and the overall assessment indicated 'No significant change in out of tolerance tests'. \"\n",
        "    report_content += \"This suggests that the adjustments made were either ineffective or insufficient to address the underlying issues completely. \"\n",
        "    report_content += \"Visualizations of critical tests still show deviations or tightly clustered values around the limits, not consistently within the stable range.\\n\"\n",
        "    report_content += \"  The goal of having all tests stable and within the 1.5% error margin has not been fully met.\"\n",
        "report_content += \"\\n\\n\"\n",
        "\n",
        "report_content += \"### Recommendations for Next Steps:\\n\"\n",
        "if is_in_calibration:\n",
        "    report_content += \"  - Approve machine for production use.\"\n",
        "    report_content += \"  - Schedule routine maintenance and performance checks to maintain calibration integrity.\"\n",
        "    report_content += \"  - Archive current calibration data as a baseline.\"\n",
        "else:\n",
        "    report_content += \"  - Re-evaluate the calibration procedure and identify factors contributing to the persistent 'out of tolerance' tests.\"\n",
        "    report_content += \"  - Perform additional adjustments focusing on the identified critical tests (e.g., SAME_87, SAME_88, VFBC, etc.).\"\n",
        "    report_content += \"  - Conduct another full calibration cycle and re-analyze the results.\"\n",
        "    report_content += \"  - Potentially consult with equipment manufacturers for deeper diagnostic insights.\"\n",
        "\n",
        "\n",
        "# 3. Create a unique filename\n",
        "report_filename = f\"comparative_calibration_report_{timestamp}_{engineer_name.replace(' ', '_')}.txt\"\n",
        "\n",
        "# 4. Save the constructed report string to the file\n",
        "with open(report_filename, 'w') as f:\n",
        "    f.write(report_content)\n",
        "\n",
        "# 5. Print a confirmation message and preview\n",
        "print(f\"Comparative calibration report saved to: {report_filename}\")\n",
        "print(\"\\n--- Report Content Preview ---\\n\")\n",
        "print(report_content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8add9e12"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "Is the machine considered \"in calibration\" after the performed adjustments?\n",
        "No, the machine is not considered \"in calibration\" after the adjustments. The analysis concluded that \"further calibration is suggested\" due to the persistence of 20 \"out of tolerance\" tests and no significant improvement observed between the initial and adjusted calibration states.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A robust `load_calibration_data` Python function was successfully developed to handle complex CSV structures, dynamically extract unique column headers (test names) from the 16th line of the CSV, and separate calibration measurements, limit values, and limit directions. Initial parsing issues were resolved by correctly identifying the number of header rows (`nrows=4`) and their respective row indices within the `header_info_df` to extract test names from `iloc[1, 2:]`, limit values from `iloc[2, 2:]`, and limit directions from `iloc[3, 2:]`.\n",
        "*   **Initial Calibration Analysis**: The initial calibration data showed an \"out of tolerance\" overall status. Out of a total of 3570 measurements analyzed, 3480 were 'stable', 70 were 'close to limit', and **20 were 'out of tolerance'**. The top 10 most critical tests (based on maximum deviation) included 'SAME_87', 'SAME_88', 'SAME_89', 'SAME_90', 'SAME_91', 'SAME_92', 'VFBC', 'VFBC_3', 'VFBC_2', and 'VFBC_4'.\n",
        "*   **Adjusted Calibration Analysis**: When the same analysis was applied to the \"adjusted\" calibration data (which, for demonstration purposes, used the same CSV file as the initial data), the results were identical. The overall status was still \"out of tolerance\", with the same counts of 'stable' (3480), 'close to limit' (70), and 'out of tolerance' (20) measurements. The top 10 critical tests also remained the same.\n",
        "*   **Comparative Analysis**: The comparison between the initial and \"adjusted\" calibrations showed **no change** in measurement shifts, 'out of tolerance' counts, or 'close to limit' counts. The average absolute measurement shift was 0.0. The overall assessment indicated \"No significant change in 'out of tolerance' tests. Further analysis needed.\"\n",
        "*   **Comparative Visualizations**: Side-by-side box plots for the 10 most critical tests for both initial and adjusted states appeared identical, visually confirming the lack of change between the two datasets.\n",
        "*   **Final Comparative Report**: The final report concluded that \"further calibration is suggested\" because the adjustments did not resolve the \"out of tolerance\" conditions.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   **Insight**: The current \"adjusted\" calibration process (or the simulated adjusted data) did not yield any improvement. The machine still has 20 measurements \"out of tolerance\", indicating that the adjustments were either ineffective or the process for generating adjusted data needs to accurately reflect actual changes.\n",
        "*   **Next Steps**: It is crucial to re-evaluate the calibration procedure and apply it to an *actually adjusted* dataset. Subsequently, re-run the comparative analysis to assess whether true recalibration efforts have successfully reduced the number of \"out of tolerance\" tests and brought the machine within acceptable performance parameters.\n"
      ]
    }
  ]
}